{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q tf-models-official==2.3.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "os.environ[\"TFHUB_CACHE_DIR\"] = \"gs://nts2020-tpu\"\n",
    "\n",
    "from official import nlp\n",
    "from official.modeling import tf_utils\n",
    "from official.nlp import bert\n",
    "\n",
    "# Load the required submodules\n",
    "import official.nlp.optimization\n",
    "import official.nlp.bert.bert_models\n",
    "import official.nlp.bert.configs\n",
    "import official.nlp.bert.run_classifier\n",
    "import official.nlp.bert.tokenization\n",
    "import official.nlp.data.classifier_data_lib\n",
    "import official.nlp.modeling.losses\n",
    "import official.nlp.modeling.models\n",
    "import official.nlp.modeling.networks\n",
    "\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:TPU system tpu-quickstart has already been initialized. Reinitializing the TPU can cause previously created variables on TPU to be lost.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:TPU system tpu-quickstart has already been initialized. Reinitializing the TPU can cause previously created variables on TPU to be lost.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Initializing the TPU system: tpu-quickstart\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Initializing the TPU system: tpu-quickstart\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Clearing out eager caches\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Clearing out eager caches\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Finished initializing TPU system.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Finished initializing TPU system.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on TPU  ['10.201.178.138:8470']\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "  tpu = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='tpu-quickstart')\n",
    "  tf.config.experimental_connect_to_cluster(tpu)\n",
    "  tf.tpu.experimental.initialize_tpu_system(tpu)# TPU detection\n",
    "  print('Running on TPU ', tpu.cluster_spec().as_dict()['worker'])\n",
    "except ValueError:\n",
    "  raise BaseException('ERROR: Not connected to a TPU runtime; please see the previous cell in this notebook for instructions!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.3.0'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_file_dataset(input_file, name_to_features, num_samples=None):\n",
    "  \"\"\"Creates a single-file dataset to be passed for BERT custom training.\"\"\"\n",
    "  # For training, we want a lot of parallel reading and shuffling.\n",
    "  # For eval, we want no shuffling and parallel reading doesn't matter.\n",
    "  d = tf.data.TFRecordDataset(input_file)\n",
    "  if num_samples:\n",
    "    d = d.take(num_samples)\n",
    "  d = d.map(\n",
    "      lambda record: decode_record(record, name_to_features),\n",
    "      num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "  # When `input_file` is a path to a single file or a list\n",
    "  # containing a single path, disable auto sharding so that\n",
    "  # same input file is sent to all workers.\n",
    "  if isinstance(input_file, str) or len(input_file) == 1:\n",
    "    options = tf.data.Options()\n",
    "    options.experimental_distribute.auto_shard_policy = (\n",
    "        tf.data.experimental.AutoShardPolicy.OFF)\n",
    "    d = d.with_options(options)\n",
    "  return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_record(record, name_to_features):\n",
    "  \"\"\"Decodes a record to a TensorFlow example.\"\"\"\n",
    "  example = tf.io.parse_single_example(record, name_to_features)\n",
    "\n",
    "  # tf.Example only supports tf.int64, but the TPU only supports tf.int32.\n",
    "  # So cast all int64 to int32.\n",
    "  for name in list(example.keys()):\n",
    "    t = example[name]\n",
    "    if t.dtype == tf.int64:\n",
    "      t = tf.cast(t, tf.int32)\n",
    "    example[name] = t\n",
    "\n",
    "  return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_classifier_dataset(file_path,\n",
    "                              seq_length,\n",
    "                              batch_size,\n",
    "                              task_id,\n",
    "                              is_training=True,\n",
    "                              input_pipeline_context=None,\n",
    "                              label_type=tf.int64,\n",
    "                              lang_id = 0,\n",
    "                              include_sample_weights=False,\n",
    "                              num_samples=None):\n",
    "  \"\"\"Creates input dataset from (tf)records files for train/eval.\"\"\"\n",
    "  name_to_features = {\n",
    "      'input_ids': tf.io.FixedLenFeature([seq_length], tf.int64),\n",
    "      'input_mask': tf.io.FixedLenFeature([seq_length], tf.int64),\n",
    "      'segment_ids': tf.io.FixedLenFeature([seq_length], tf.int64),\n",
    "      'label_ids': tf.io.FixedLenFeature([], label_type),\n",
    "  }\n",
    "  if include_sample_weights:\n",
    "    name_to_features['weight'] = tf.io.FixedLenFeature([], tf.float32)\n",
    "  dataset = single_file_dataset(file_path, name_to_features,\n",
    "                                num_samples=num_samples)\n",
    "\n",
    "  # The dataset is always sharded by number of hosts.\n",
    "  # num_input_pipelines is the number of hosts rather than number of cores.\n",
    "  if input_pipeline_context and input_pipeline_context.num_input_pipelines > 1:\n",
    "    dataset = dataset.shard(input_pipeline_context.num_input_pipelines,\n",
    "                            input_pipeline_context.input_pipeline_id)\n",
    "\n",
    "  def _select_data_from_record(record):\n",
    "    x = {\n",
    "        'input_word_ids': record['input_ids'],\n",
    "        'input_mask': record['input_mask'],\n",
    "        'input_type_ids': record['segment_ids'],\n",
    "         'lang_id' : lang_id\n",
    "    }\n",
    "    #pdb.set_trace()\n",
    "    y = record['label_ids']\n",
    "    if include_sample_weights:\n",
    "      w = record['weight']\n",
    "      return (x, y, w)\n",
    "    default = tf.constant(-1, dtype=tf.int32)\n",
    "    if task_id ==0:\n",
    "      return (x, (y, default))\n",
    "    if task_id == 1:\n",
    "      return (x, (default,y))\n",
    "\n",
    "  if is_training:\n",
    "    dataset = dataset.shuffle(100)\n",
    "    dataset = dataset.repeat()\n",
    "\n",
    "  dataset = dataset.map(\n",
    "      _select_data_from_record,\n",
    "      num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "  #dataset = dataset.batch(batch_size, drop_remainder=is_training)\n",
    "  dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "  return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_dict = {\n",
    "\"attention_probs_dropout_prob\": 0.1,\n",
    "\"directionality\": \"bidi\",\n",
    "\"hidden_act\": \"gelu\",\n",
    "\"hidden_dropout_prob\": 0.1,\n",
    "\"hidden_size\": 768,\n",
    "\"initializer_range\": 0.02,\n",
    "\"intermediate_size\": 3072,\n",
    "\"max_position_embeddings\": 512,\n",
    "\"num_attention_heads\": 12,\n",
    "\"num_hidden_layers\": 12,\n",
    "\"pooler_fc_size\": 768,\n",
    "\"pooler_num_attention_heads\": 12,\n",
    "\"pooler_num_fc_layers\": 3,\n",
    "\"pooler_size_per_head\": 128,\n",
    "\"pooler_type\": \"first_token_transform\",\n",
    "\"type_vocab_size\": 2,\n",
    "\"vocab_size\": 119547\n",
    "}\n",
    "\n",
    "bert_config = bert.configs.BertConfig.from_dict(config_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-9-96169b7cdfc6>:6: tf_record_iterator (from tensorflow.python.lib.io.tf_record) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use eager execution and: \n",
      "`tf.data.TFRecordDataset(path)`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-9-96169b7cdfc6>:6: tf_record_iterator (from tensorflow.python.lib.io.tf_record) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use eager execution and: \n",
      "`tf.data.TFRecordDataset(path)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49401\n",
      "392702\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.11174092914999446, 0.8882590708500055]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_records_filenames = [\"gs://nts2020/xtereme/pawsx/train.en.tfrecords\", \"gs://nts2020/xtereme/xnli/train.en.tfrecords\"]\n",
    "\n",
    "sampling_factor = []\n",
    "for fn in tf_records_filenames:\n",
    "    c = 0\n",
    "    for record in tf.compat.v1.python_io.tf_record_iterator(fn):\n",
    "        c += 1\n",
    "    sampling_factor.append(c)\n",
    "    print(c)\n",
    "c = sum(sampling_factor)\n",
    "for i in range(0, len(sampling_factor)):\n",
    "    sampling_factor[i] = sampling_factor[i]/c\n",
    "sampling_factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_records_filenames = [\"gs://nts2020/xtreme/translate_train/train.ar.tfrecords\", \"gs://nts2020/xtreme/translate_train/train.bg.tfrecords\", \"gs://nts2020/xtreme/translate_train/train.de.tfrecords\",\n",
    "                        \"gs://nts2020/xtreme/translate_train/train.el.tfrecords\",\"gs://nts2020/xtreme/translate_train/train.es.tfrecords\",\"gs://nts2020/xtreme/translate_train/train.fr.tfrecords\",\n",
    "                        \"gs://nts2020/xtreme/translate_train/train.hi.tfrecords\",\"gs://nts2020/xtreme/translate_train/train.ru.tfrecords\",\"gs://nts2020/xtreme/translate_train/train.sw.tfrecords\",\n",
    "                        \"gs://nts2020/xtreme/translate_train/train.th.tfrecords\",\"gs://nts2020/xtreme/translate_train/train.tr.tfrecords\",\"gs://nts2020/xtreme/translate_train/train.ur.tfrecords\",\n",
    "                        \"gs://nts2020/xtreme/translate_train/train.vi.tfrecords\",\"gs://nts2020/xtreme/translate_train/train.zh.tfrecords\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "392702\n",
      "392702\n",
      "392702\n",
      "392702\n",
      "392702\n",
      "392702\n",
      "392702\n",
      "392702\n",
      "392702\n",
      "392702\n",
      "392702\n",
      "392702\n",
      "392702\n",
      "392702\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.07142857142857142,\n",
       " 0.07142857142857142,\n",
       " 0.07142857142857142,\n",
       " 0.07142857142857142,\n",
       " 0.07142857142857142,\n",
       " 0.07142857142857142,\n",
       " 0.07142857142857142,\n",
       " 0.07142857142857142,\n",
       " 0.07142857142857142,\n",
       " 0.07142857142857142,\n",
       " 0.07142857142857142,\n",
       " 0.07142857142857142,\n",
       " 0.07142857142857142,\n",
       " 0.07142857142857142]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_records_filenames = [\"gs://nts2020/xtreme/translate_train/train.ar.tfrecords\", \"gs://nts2020/xtreme/translate_train/train.bg.tfrecords\", \"gs://nts2020/xtreme/translate_train/train.de.tfrecords\",\n",
    "                        \"gs://nts2020/xtreme/translate_train/train.el.tfrecords\",\"gs://nts2020/xtreme/translate_train/train.es.tfrecords\",\"gs://nts2020/xtreme/translate_train/train.fr.tfrecords\",\n",
    "                        \"gs://nts2020/xtreme/translate_train/train.hi.tfrecords\",\"gs://nts2020/xtreme/translate_train/train.ru.tfrecords\",\"gs://nts2020/xtreme/translate_train/train.sw.tfrecords\",\n",
    "                        \"gs://nts2020/xtreme/translate_train/train.th.tfrecords\",\"gs://nts2020/xtreme/translate_train/train.tr.tfrecords\",\"gs://nts2020/xtreme/translate_train/train.ur.tfrecords\",\n",
    "                        \"gs://nts2020/xtreme/translate_train/train.vi.tfrecords\",\"gs://nts2020/xtreme/translate_train/train.zh.tfrecords\"]\n",
    "\n",
    "other_langs_sampling_factor = []\n",
    "for fn in tf_records_filenames:\n",
    "    c = 0\n",
    "    for record in tf.compat.v1.python_io.tf_record_iterator(fn):\n",
    "        c += 1\n",
    "    other_langs_sampling_factor.append(c)\n",
    "    print(c)\n",
    "c = sum(other_langs_sampling_factor)\n",
    "for i in range(0, len(other_langs_sampling_factor)):\n",
    "    other_langs_sampling_factor[i] = other_langs_sampling_factor[i]/c\n",
    "other_langs_sampling_factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "other_lang_count = len(tf_records_filenames)\n",
    "other_lang_aggregate_weight = 0.9\n",
    "train_sampling_factor = []\n",
    "for i in sampling_factor:\n",
    "  train_sampling_factor.append((i* (1-other_lang_aggregate_weight))/ sum(sampling_factor))\n",
    "for i in other_langs_sampling_factor:\n",
    "  train_sampling_factor.append((i * other_lang_aggregate_weight))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sampling_factor[0] = 0.03"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sampling_factor = [0.03,\n",
    " 0.08882590708500053,\n",
    " 0.06428571428571428,\n",
    " 0.06428571428571428,\n",
    " 0.06428571428571428,\n",
    " 0.06428571428571428,\n",
    " 0.06428571428571428,\n",
    " 0.06428571428571428,\n",
    " 0.06428571428571428,\n",
    " 0.06428571428571428,\n",
    " 0.06428571428571428,\n",
    " 0.06428571428571428,\n",
    " 0.06428571428571428,\n",
    " 0.06428571428571428,\n",
    " 0.06428571428571428,\n",
    " 0.06428571428571428]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "if sum(train_sampling_factor)!=1:\n",
    "  train_sampling_factor[1]+= 1- sum(train_sampling_factor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _loss_with_filter(y_true, y_pred):\n",
    "  num_labels = y_pred.get_shape().as_list()[-1]\n",
    "  log_probs = tf.nn.log_softmax(y_pred, axis=-1)\n",
    "  log_probs = tf.reshape(log_probs, [-1, num_labels])\n",
    "  labels = tf.reshape(y_true, [-1])\n",
    "  one_hot_labels = tf.one_hot(labels, depth=num_labels, dtype=tf.float32)\n",
    "  per_example_loss = -tf.reduce_sum(one_hot_labels * log_probs, axis=-1)\n",
    "  loss = tf.reduce_mean(per_example_loss)\n",
    "  return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras.backend as K\n",
    "def accuracy_mod(y_true, y_pred):\n",
    "  # Squeeze the shape to (None, ) from (None, 1) as we want to apply operations directly on y_true\n",
    "  if K.ndim(y_true) == K.ndim(y_pred):\n",
    "        y_true = K.squeeze(y_true, -1)\n",
    "\n",
    "  # Normalize the y_pred values first and then take the arg at which we have a maximum value (This is the predicted label)\n",
    "  y_pred = K.softmax(y_pred, axis = -1)\n",
    "  y_pred = K.argmax(y_pred, axis = -1)\n",
    "\n",
    "  # Since the ground labels can also have -1s for which we don't wanna calculate accuracy, we are filtering them off\n",
    "  defa = K.constant([0], dtype=tf.float32)\n",
    "  #Creating a boolean tensor for labels greater or equal to 0\n",
    "  is_valid = K.greater_equal(y_true, defa)\n",
    "  #Get the corresponding indices\n",
    "  indices = tf.where(is_valid)\n",
    "\n",
    "  #Gather the results of y_true and y_pred at the indices we calculated above\n",
    "  fil_y_true = K.gather(y_true, K.reshape(indices, [-1])) \n",
    "  fil_y_pred = K.gather(y_pred, K.reshape(indices, [-1]))\n",
    "  # K.print_tensor(res, message='res = ')\n",
    "  # K.print_tensor(comp, message='comp = ')\n",
    "\n",
    "  fil_y_true = K.cast(fil_y_true, K.floatx())\n",
    "  fil_y_pred = K.cast(fil_y_pred, K.floatx())\n",
    "\n",
    "  #pdb.set_trace()\n",
    "  return K.cast(K.equal(fil_y_true, fil_y_pred), K.floatx())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 3\n",
    "batch_size = 64\n",
    "eval_batch_size = 64\n",
    "max_seq_length = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "paws_training_dataset = create_classifier_dataset(\n",
    "\"gs://nts2020/xtereme/pawsx/train.en.tfrecords\",\n",
    "128,\n",
    "batch_size,\n",
    "task_id = 0,\n",
    "is_training=True)\n",
    "\n",
    "\n",
    "xnli_training_dataset = create_classifier_dataset(\n",
    "\"gs://nts2020/xtereme/xnli/train.en.tfrecords\",\n",
    "128,\n",
    "batch_size,\n",
    "task_id =1,\n",
    "is_training=True)\n",
    "\n",
    "paws_eval_dataset = create_classifier_dataset(\n",
    "\"gs://nts2020/xtereme/pawsx/eval.en.tfrecords\",\n",
    "128,\n",
    "batch_size,\n",
    "task_id = 0,\n",
    "is_training=False)\n",
    "\n",
    "xnli_eval_dataset = create_classifier_dataset(\n",
    "\"gs://nts2020/xtereme/xnli/eval.en.tfrecords\",\n",
    "128,\n",
    "batch_size,\n",
    "task_id = 1,\n",
    "is_training=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_records_filenames = [\"gs://nts2020/xtreme/translate_train/train.ar.tfrecords\", \"gs://nts2020/xtreme/translate_train/train.bg.tfrecords\", \"gs://nts2020/xtreme/translate_train/train.de.tfrecords\",\n",
    "                        \"gs://nts2020/xtreme/translate_train/train.el.tfrecords\",\"gs://nts2020/xtreme/translate_train/train.es.tfrecords\",\"gs://nts2020/xtreme/translate_train/train.fr.tfrecords\",\n",
    "                        \"gs://nts2020/xtreme/translate_train/train.hi.tfrecords\",\"gs://nts2020/xtreme/translate_train/train.ru.tfrecords\",\"gs://nts2020/xtreme/translate_train/train.sw.tfrecords\",\n",
    "                        \"gs://nts2020/xtreme/translate_train/train.th.tfrecords\",\"gs://nts2020/xtreme/translate_train/train.tr.tfrecords\",\"gs://nts2020/xtreme/translate_train/train.ur.tfrecords\",\n",
    "                        \"gs://nts2020/xtreme/translate_train/train.vi.tfrecords\",\"gs://nts2020/xtreme/translate_train/train.zh.tfrecords\"]\n",
    "xnli_ar_training_dataset = create_classifier_dataset(\n",
    "    \"gs://nts2020/xtreme/translate_train/train.ar.tfrecords\",\n",
    "    128,\n",
    "    batch_size,\n",
    "    task_id =1,\n",
    "    lang_id =1,\n",
    "    is_training=True)\n",
    "xnli_bg_training_dataset = create_classifier_dataset(\n",
    "    \"gs://nts2020/xtreme/translate_train/train.bg.tfrecords\",\n",
    "    128,\n",
    "    batch_size,\n",
    "    task_id =1,\n",
    "    lang_id =2,\n",
    "    is_training=True)\n",
    "xnli_de_training_dataset = create_classifier_dataset(\n",
    "    \"gs://nts2020/xtreme/translate_train/train.de.tfrecords\",\n",
    "    128,\n",
    "    batch_size,\n",
    "    task_id =1,\n",
    "    lang_id =3,\n",
    "    is_training=True)\n",
    "xnli_el_training_dataset = create_classifier_dataset(\n",
    "    \"gs://nts2020/xtreme/translate_train/train.el.tfrecords\",\n",
    "    128,\n",
    "    batch_size,\n",
    "    task_id =1,\n",
    "    lang_id =4,\n",
    "    is_training=True)\n",
    "xnli_es_training_dataset = create_classifier_dataset(\n",
    "    \"gs://nts2020/xtreme/translate_train/train.es.tfrecords\",\n",
    "    128,\n",
    "    batch_size,\n",
    "    task_id =1,\n",
    "    lang_id =5,\n",
    "    is_training=True)\n",
    "xnli_fr_training_dataset = create_classifier_dataset(\n",
    "    \"gs://nts2020/xtreme/translate_train/train.fr.tfrecords\",\n",
    "    128,\n",
    "    batch_size,\n",
    "    task_id =1,\n",
    "    lang_id =6,\n",
    "    is_training=True)\n",
    "xnli_hi_training_dataset = create_classifier_dataset(\n",
    "    \"gs://nts2020/xtreme/translate_train/train.hi.tfrecords\",\n",
    "    128,\n",
    "    batch_size,\n",
    "    task_id =1,\n",
    "    lang_id =7,\n",
    "    is_training=True)\n",
    "xnli_ru_training_dataset = create_classifier_dataset(\n",
    "    \"gs://nts2020/xtreme/translate_train/train.ru.tfrecords\",\n",
    "    128,\n",
    "    batch_size,\n",
    "    task_id =1,\n",
    "    lang_id =8,\n",
    "    is_training=True)\n",
    "xnli_sw_training_dataset = create_classifier_dataset(\n",
    "    \"gs://nts2020/xtreme/translate_train/train.sw.tfrecords\",\n",
    "    128,\n",
    "    batch_size,\n",
    "    task_id =1,\n",
    "    lang_id =9,\n",
    "    is_training=True)\n",
    "xnli_th_training_dataset = create_classifier_dataset(\n",
    "    \"gs://nts2020/xtreme/translate_train/train.th.tfrecords\",\n",
    "    128,\n",
    "    batch_size,\n",
    "    task_id =1,\n",
    "    lang_id =10,\n",
    "    is_training=True)\n",
    "xnli_tr_training_dataset = create_classifier_dataset(\n",
    "    \"gs://nts2020/xtreme/translate_train/train.tr.tfrecords\",\n",
    "    128,\n",
    "    batch_size,\n",
    "    task_id =1,\n",
    "    lang_id =11,\n",
    "    is_training=True)\n",
    "xnli_ur_training_dataset = create_classifier_dataset(\n",
    "    \"gs://nts2020/xtreme/translate_train/train.ur.tfrecords\",\n",
    "    128,\n",
    "    batch_size,\n",
    "    task_id =1,\n",
    "    lang_id =12,\n",
    "    is_training=True)\n",
    "xnli_vi_training_dataset = create_classifier_dataset(\n",
    "    \"gs://nts2020/xtreme/translate_train/train.vi.tfrecords\",\n",
    "    128,\n",
    "    batch_size,\n",
    "    task_id =1,\n",
    "    lang_id =13,\n",
    "    is_training=True)\n",
    "xnli_zh_training_dataset = create_classifier_dataset(\n",
    "    \"gs://nts2020/xtreme/translate_train/train.zh.tfrecords\",\n",
    "    128,\n",
    "    batch_size,\n",
    "    task_id =1,\n",
    "    lang_id =14,\n",
    "    is_training=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "xnli_ar_eval_dataset = create_classifier_dataset(\n",
    "    \"gs://nts2020/xtreme/xnli_w_dev/eval_ar.tfrecords\",\n",
    "    max_seq_length,\n",
    "    batch_size,\n",
    "    task_id =1,\n",
    "    is_training=False)\n",
    "xnli_bg_eval_dataset = create_classifier_dataset(\n",
    "    \"gs://nts2020/xtreme/xnli_w_dev/eval_bg.tfrecords\",\n",
    "    max_seq_length,\n",
    "    batch_size,\n",
    "    task_id =1,\n",
    "    is_training=False)\n",
    "xnli_de_eval_dataset = create_classifier_dataset(\n",
    "    \"gs://nts2020/xtreme/xnli_w_dev/eval_de.tfrecords\",\n",
    "    max_seq_length,\n",
    "    batch_size,\n",
    "    task_id =1,\n",
    "    is_training=False)\n",
    "xnli_el_eval_dataset = create_classifier_dataset(\n",
    "    \"gs://nts2020/xtreme/xnli_w_dev/eval_el.tfrecords\",\n",
    "    max_seq_length,\n",
    "    batch_size,\n",
    "    task_id =1,\n",
    "    is_training=False)\n",
    "xnli_es_eval_dataset = create_classifier_dataset(\n",
    "    \"gs://nts2020/xtreme/xnli_w_dev/eval_es.tfrecords\",\n",
    "    max_seq_length,\n",
    "    batch_size,\n",
    "    task_id =1,\n",
    "    is_training=False)\n",
    "xnli_fr_eval_dataset = create_classifier_dataset(\n",
    "    \"gs://nts2020/xtreme/xnli_w_dev/eval_fr.tfrecords\",\n",
    "    max_seq_length,\n",
    "    batch_size,\n",
    "    task_id =1,\n",
    "    is_training=False)\n",
    "xnli_hi_eval_dataset = create_classifier_dataset(\n",
    "    \"gs://nts2020/xtreme/xnli_w_dev/eval_hi.tfrecords\",\n",
    "    max_seq_length,\n",
    "    batch_size,\n",
    "    task_id =1,\n",
    "    is_training=False)\n",
    "xnli_ru_eval_dataset = create_classifier_dataset(\n",
    "    \"gs://nts2020/xtreme/xnli_w_dev/eval_ru.tfrecords\",\n",
    "    max_seq_length,\n",
    "    batch_size,\n",
    "    task_id =1,\n",
    "    is_training=False)\n",
    "xnli_sw_eval_dataset = create_classifier_dataset(\n",
    "    \"gs://nts2020/xtreme/xnli_w_dev/eval_sw.tfrecords\",\n",
    "    max_seq_length,\n",
    "    batch_size,\n",
    "    task_id =1,\n",
    "    is_training=False)\n",
    "xnli_th_eval_dataset = create_classifier_dataset(\n",
    "    \"gs://nts2020/xtreme/xnli_w_dev/eval_th.tfrecords\",\n",
    "    max_seq_length,\n",
    "    batch_size,\n",
    "    task_id =1,\n",
    "    is_training=False)\n",
    "xnli_tr_eval_dataset = create_classifier_dataset(\n",
    "    \"gs://nts2020/xtreme/xnli_w_dev/eval_tr.tfrecords\",\n",
    "    max_seq_length,\n",
    "    batch_size,\n",
    "    task_id =1,\n",
    "    is_training=False)\n",
    "xnli_ur_eval_dataset = create_classifier_dataset(\n",
    "    \"gs://nts2020/xtreme/xnli_w_dev/eval_ur.tfrecords\",\n",
    "    max_seq_length,\n",
    "    batch_size,\n",
    "    task_id =1,\n",
    "    is_training=False)\n",
    "xnli_vi_eval_dataset = create_classifier_dataset(\n",
    "    \"gs://nts2020/xtreme/xnli_w_dev/eval_vi.tfrecords\",\n",
    "    max_seq_length,\n",
    "    batch_size,\n",
    "    task_id =1,\n",
    "    is_training=False)\n",
    "xnli_zh_eval_dataset = create_classifier_dataset(\n",
    "    \"gs://nts2020/xtreme/xnli_w_dev/eval_zh.tfrecords\",\n",
    "    max_seq_length,\n",
    "    batch_size,\n",
    "    task_id =1,\n",
    "    is_training=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delta = 0.50\n",
    "\n",
    "for i in range(0, len(train_sampling_factor)):\n",
    "    train_sampling_factor[i] = train_sampling_factor[i] * (1 - delta)\n",
    "train_sampling_factor[13] += delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "if sum(train_sampling_factor)!=1:\n",
    "  train_sampling_factor[1]+= 1- sum(train_sampling_factor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sampling_factor = 16 * [None]\n",
    "train_sampling_factor[0] = 0.01\n",
    "train_sampling_factor[1:] = [0.54957585, 0.00897124, 0.00887785, 0.00799495, 0.0105019 ,\n",
    "       0.00910511, 0.00809789, 0.33139596, 0.00878845, 0.0100313 ,\n",
    "       0.00971826, 0.00991482, 0.00882816, 0.00819859, 0.00999967]\n",
    "\n",
    "\n",
    "train_sampling_factor[1] -= 0.04\n",
    "if sum(train_sampling_factor)!=1:\n",
    "  train_sampling_factor[1]+= 1- sum(train_sampling_factor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sampling_factor = [0.03,\n",
    " 0.30,\n",
    " 0.06428571428571428,\n",
    " 0.06428571428571428,\n",
    " 0.06428571428571428,\n",
    " 0.06428571428571428,\n",
    " 0.06428571428571428,\n",
    " 0.06428571428571428,\n",
    " 0.50,\n",
    " 0.06428571428571428,\n",
    " 0.06428571428571428,\n",
    " 0.06428571428571428,\n",
    " 0.06428571428571428,\n",
    " 0.06428571428571428,\n",
    " 0.06428571428571428,\n",
    " 0.06428571428571428]\n",
    "if sum(train_sampling_factor)!=1:\n",
    "  train_sampling_factor[1]+= 1- sum(train_sampling_factor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dataset = tf.data.experimental.sample_from_datasets(\n",
    "    [paws_training_dataset, xnli_training_dataset, xnli_ar_training_dataset, xnli_bg_training_dataset, xnli_de_training_dataset, xnli_el_training_dataset, xnli_es_training_dataset, \n",
    "     xnli_fr_training_dataset, xnli_hi_training_dataset, xnli_ru_training_dataset, xnli_sw_training_dataset, xnli_th_training_dataset,\n",
    "     xnli_tr_training_dataset, xnli_ur_training_dataset, xnli_vi_training_dataset, xnli_zh_training_dataset], weights=tf.constant([train_sampling_factor[0], train_sampling_factor[1],\n",
    "                                                                                                                                   train_sampling_factor[2], train_sampling_factor[3],\n",
    "                                                                                                                                   train_sampling_factor[4],train_sampling_factor[5],\n",
    "                                                                                                                                   train_sampling_factor[6],train_sampling_factor[7],\n",
    "                                                                                                                                   train_sampling_factor[8],train_sampling_factor[9],\n",
    "                                                                                                                                   train_sampling_factor[10],train_sampling_factor[11],\n",
    "                                                                                                                                   train_sampling_factor[12],train_sampling_factor[13],\n",
    "                                                                                                                                   train_sampling_factor[14],train_sampling_factor[15]]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterator = training_dataset.as_numpy_iterator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training_dataset = tf.data.experimental.sample_from_datasets(\n",
    "#     [paws_training_dataset, xnli_sw_training_dataset], weights = tf.constant([0.5,0.5]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_sampling_factor = [0.3 , 0.7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluation_dataset = tf.data.experimental.sample_from_datasets(\n",
    "#     [paws_eval_dataset, xnli_eval_dataset, xnli_ar_eval_dataset, xnli_bg_eval_dataset, xnli_de_eval_dataset, xnli_el_eval_dataset, xnli_es_eval_dataset, \n",
    "#      xnli_fr_eval_dataset, xnli_hi_eval_dataset, xnli_ru_eval_dataset, xnli_sw_eval_dataset, xnli_th_eval_dataset,\n",
    "#      xnli_tr_eval_dataset, xnli_ur_eval_dataset, xnli_vi_eval_dataset, xnli_zh_eval_dataset], weights=tf.constant([train_sampling_factor[0], train_sampling_factor[1],\n",
    "#                                                                                                                                    train_sampling_factor[2], train_sampling_factor[3],\n",
    "#                                                                                                                                    train_sampling_factor[4],train_sampling_factor[5],\n",
    "#                                                                                                                                    train_sampling_factor[6],train_sampling_factor[7],\n",
    "#                                                                                                                                    train_sampling_factor[8],train_sampling_factor[9],\n",
    "#                                                                                                                                    train_sampling_factor[10],train_sampling_factor[11],\n",
    "#                                                                                                                                    train_sampling_factor[12],train_sampling_factor[13],\n",
    "#                                                                                                                                    train_sampling_factor[14],train_sampling_factor[15]]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_dataset = tf.data.experimental.sample_from_datasets(\n",
    "    [paws_eval_dataset,xnli_hi_eval_dataset], weights=tf.constant([evaluation_sampling_factor[0],evaluation_sampling_factor[1]]))                                                                                                                              \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluation_dataset = tf.data.experimental.sample_from_datasets(\n",
    "#     [paws_eval_dataset, xnli_eval_dataset], weights=tf.constant([sampling_factor[0], sampling_factor[1]])\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class myCallback(tf.keras.callbacks.Callback): \n",
    "    def on_epoch_end(self, epoch, logs={}): \n",
    "        if(logs.get('val_output1_accuracy_mod') > 0.71):   \n",
    "          print(\"\\nWe have reached %2.2f%% accuracy, so we will stopping training.\" %(acc_thresh*100))   \n",
    "          self.model.stop_training = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = myCallback()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Found TPU system:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Found TPU system:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Num TPU Cores: 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Num TPU Cores: 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Num TPU Workers: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Num TPU Workers: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/35\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sundard/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:432: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   2/1000 [..............................] - ETA: 56s - loss: 1.0946 - output1_loss: 0.0126 - output2_loss: 1.0820 - output1_accuracy_mod: 0.0000e+00 - output2_accuracy_mod: 0.3175WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0115s vs `on_train_batch_end` time: 0.0950s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0115s vs `on_train_batch_end` time: 0.0950s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 111s 111ms/step - loss: 1.0890 - output1_loss: 0.0151 - output2_loss: 1.0739 - output1_accuracy_mod: 0.5642 - output2_accuracy_mod: 0.3620 - val_loss: 0.9030 - val_output1_loss: 0.3042 - val_output2_loss: 0.5988 - val_output1_accuracy_mod: 0.5643 - val_output2_accuracy_mod: 0.4112\n",
      "Epoch 2/35\n",
      "1000/1000 [==============================] - 107s 107ms/step - loss: 0.9475 - output1_loss: 0.0155 - output2_loss: 0.9320 - output1_accuracy_mod: 0.5416 - output2_accuracy_mod: 0.5437 - val_loss: 0.8276 - val_output1_loss: 0.3044 - val_output2_loss: 0.5232 - val_output1_accuracy_mod: 0.5633 - val_output2_accuracy_mod: 0.5550\n",
      "Epoch 3/35\n",
      "1000/1000 [==============================] - 109s 109ms/step - loss: 0.8540 - output1_loss: 0.0147 - output2_loss: 0.8393 - output1_accuracy_mod: 0.5431 - output2_accuracy_mod: 0.6115 - val_loss: 0.8151 - val_output1_loss: 0.3041 - val_output2_loss: 0.5110 - val_output1_accuracy_mod: 0.5693 - val_output2_accuracy_mod: 0.5795\n",
      "Epoch 4/35\n",
      "1000/1000 [==============================] - 110s 110ms/step - loss: 0.8105 - output1_loss: 0.0151 - output2_loss: 0.7954 - output1_accuracy_mod: 0.5549 - output2_accuracy_mod: 0.6379 - val_loss: 0.7848 - val_output1_loss: 0.3038 - val_output2_loss: 0.4810 - val_output1_accuracy_mod: 0.5704 - val_output2_accuracy_mod: 0.6177\n",
      "Epoch 5/35\n",
      "1000/1000 [==============================] - 110s 110ms/step - loss: 0.7875 - output1_loss: 0.0147 - output2_loss: 0.7728 - output1_accuracy_mod: 0.5485 - output2_accuracy_mod: 0.6508 - val_loss: 0.7834 - val_output1_loss: 0.3054 - val_output2_loss: 0.4780 - val_output1_accuracy_mod: 0.5583 - val_output2_accuracy_mod: 0.6249\n",
      "Epoch 6/35\n",
      " 176/1000 [====>.........................] - ETA: 1:29 - loss: 0.7820 - output1_loss: 0.0159 - output2_loss: 0.7661 - output1_accuracy_mod: 0.5521 - output2_accuracy_mod: 0.6546"
     ]
    }
   ],
   "source": [
    "# resolver = tf.distribute.cluster_resolver.TPUClusterResolver.connect(tpu='tpu-quickstart', project = 'moana-intern-fall-2020')\n",
    "# tf.config.experimental_connect_to_cluster(resolver)\n",
    "# tf.tpu.experimental.initialize_tpu_system(resolver)\n",
    "# strategy = tf.distribute.TPUStrategy(resolver)\n",
    "\n",
    "strategy = tf.distribute.TPUStrategy(tpu)\n",
    "\n",
    "with strategy.scope():\n",
    "    max_seq_length = 128\n",
    "    initializer = tf.keras.initializers.TruncatedNormal(\n",
    "            stddev=bert_config.initializer_range)\n",
    "    bert_encoder = bert.bert_models.get_transformer_encoder(\n",
    "        bert_config, max_seq_length)\n",
    "\n",
    "    input_word_ids = tf.keras.layers.Input(\n",
    "      shape=(max_seq_length,), dtype=tf.int32, name='input_word_ids')\n",
    "    input_mask = tf.keras.layers.Input(\n",
    "      shape=(max_seq_length,), dtype=tf.int32, name='input_mask')\n",
    "    input_type_ids = tf.keras.layers.Input(\n",
    "      shape=(max_seq_length,), dtype=tf.int32, name='input_type_ids')\n",
    "\n",
    "    bert_model = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_multi_cased_L-12_H-768_A-12/2\",\n",
    "                                trainable=True)\n",
    "    #bert_model = hub.KerasLayer(hub_url_bert, trainable=True)\n",
    "    pooled_output, seq_output = bert_model([input_word_ids, input_mask, input_type_ids])\n",
    "    output1 = tf.keras.layers.Dropout(rate=bert_config.hidden_dropout_prob)(\n",
    "      pooled_output)\n",
    "\n",
    "    output1 = tf.keras.layers.Dense(\n",
    "      2, kernel_initializer=initializer, name='output1')(\n",
    "          output1)\n",
    "\n",
    "    output2 = tf.keras.layers.Dropout(rate=bert_config.hidden_dropout_prob)(\n",
    "      pooled_output)\n",
    "\n",
    "    output2 = tf.keras.layers.Dense(\n",
    "      3, kernel_initializer=initializer, name='output2')(\n",
    "          output2)\n",
    "\n",
    "    model = tf.keras.Model(\n",
    "          inputs={\n",
    "              'input_word_ids': input_word_ids,\n",
    "              'input_mask': input_mask,\n",
    "              'input_type_ids': input_type_ids\n",
    "          },\n",
    "          outputs=[output1, output2])\n",
    "\n",
    "    # Set up epochs and steps\n",
    "\n",
    "    # get train_data_size from metadata\n",
    "    train_data_size = c\n",
    "    steps_per_epoch = int(train_data_size / batch_size)\n",
    "    num_train_steps = steps_per_epoch * epochs\n",
    "    warmup_steps = int(epochs * train_data_size * 0.1 / batch_size)\n",
    "\n",
    "    # creates an optimizer with learning rate schedule\n",
    "    optimizer = nlp.optimization.create_optimizer(\n",
    "        2e-5, num_train_steps=num_train_steps, num_warmup_steps=warmup_steps)\n",
    "\n",
    "  \n",
    "    \n",
    "    \n",
    "    training_dataset = training_dataset.batch(batch_size)\n",
    "    evaluation_dataset = evaluation_dataset.batch(batch_size, drop_remainder=True)\n",
    "    \n",
    " \n",
    "\n",
    "    model.compile(optimizer = optimizer, loss = [_loss_with_filter, _loss_with_filter], metrics = [accuracy_mod])\n",
    "    history = model.fit(training_dataset, batch_size = batch_size, epochs= 35, steps_per_epoch = 1000, validation_data=evaluation_dataset, callbacks = [callbacks])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [],
   "source": [
    "xnli_hi_test_dataset = create_classifier_dataset(\n",
    "    \"gs://nts2020/xtreme/xnli_w_dev/test_hi.tf_record\",\n",
    "    max_seq_length,\n",
    "    batch_size,\n",
    "    task_id =1,\n",
    "    is_training=False)\n",
    "xnli_hi_test_dataset = xnli_hi_test_dataset.batch(batch_size, drop_remainder = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79/79 [==============================] - 7s 84ms/step - loss: nan - output1_loss: nan - output2_loss: nan - output1_accuracy_mod: 0.0000e+00 - output2_accuracy_mod: 0.7126\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[nan, nan, nan, 0.0, 0.7125748991966248]"
      ]
     },
     "execution_count": 342,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(xnli_hi_test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'input_word_ids': array([[ 101,  100, 1010, ...,    0,    0,    0],\n",
       "         [ 101,  100, 1010, ...,    0,    0,    0],\n",
       "         [ 101,  100, 1010, ...,    0,    0,    0],\n",
       "         ...,\n",
       "         [ 101,  100, 2018, ...,    0,    0,    0],\n",
       "         [ 101,  100, 2018, ...,    0,    0,    0],\n",
       "         [ 101,  100, 2215, ...,    0,    0,    0]], dtype=int32),\n",
       "  'input_mask': array([[1, 1, 1, ..., 0, 0, 0],\n",
       "         [1, 1, 1, ..., 0, 0, 0],\n",
       "         [1, 1, 1, ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [1, 1, 1, ..., 0, 0, 0],\n",
       "         [1, 1, 1, ..., 0, 0, 0],\n",
       "         [1, 1, 1, ..., 0, 0, 0]], dtype=int32),\n",
       "  'input_type_ids': array([[0, 0, 0, ..., 0, 0, 0],\n",
       "         [0, 0, 0, ..., 0, 0, 0],\n",
       "         [0, 0, 0, ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0, ..., 0, 0, 0],\n",
       "         [0, 0, 0, ..., 0, 0, 0],\n",
       "         [0, 0, 0, ..., 0, 0, 0]], dtype=int32),\n",
       "  'lang_id': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        dtype=int32)},\n",
       " (array([-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "         -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "         -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "         -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1], dtype=int32),\n",
       "  array([0, 1, 2, 2, 1, 0, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 2, 1, 0, 1, 2, 1,\n",
       "         0, 2, 2, 1, 0, 0, 2, 1, 2, 1, 0, 1, 2, 0, 0, 1, 2, 0, 2, 1, 0, 1,\n",
       "         2, 1, 2, 0, 1, 0, 2, 2, 1, 0, 1, 2, 0, 1, 2, 0, 2, 1, 0, 2],\n",
       "        dtype=int32)))"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iterat = xnli_hi_test_dataset.as_numpy_iterator()\n",
    "iterat.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "sw_impulse = create_classifier_dataset(\n",
    "    \"gs://nts2020/xtreme/xnli_w_dev/eval_sw.tfrecords\",\n",
    "    max_seq_length,\n",
    "    batch_size,\n",
    "    task_id =1,\n",
    "    is_training=False)\n",
    "sw_impulse = sw_impulse.batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'input_word_ids': array([[  101, 10685, 12871, ...,     0,     0,     0],\n",
       "         [  101, 10685, 12871, ...,     0,     0,     0],\n",
       "         [  101, 10685, 12871, ...,     0,     0,     0],\n",
       "         ...,\n",
       "         [  101,   138, 12964, ...,     0,     0,     0],\n",
       "         [  101,   138, 12964, ...,     0,     0,     0],\n",
       "         [  101, 48511, 25327, ...,     0,     0,     0]], dtype=int32),\n",
       "  'input_mask': array([[1, 1, 1, ..., 0, 0, 0],\n",
       "         [1, 1, 1, ..., 0, 0, 0],\n",
       "         [1, 1, 1, ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [1, 1, 1, ..., 0, 0, 0],\n",
       "         [1, 1, 1, ..., 0, 0, 0],\n",
       "         [1, 1, 1, ..., 0, 0, 0]], dtype=int32),\n",
       "  'input_type_ids': array([[0, 0, 0, ..., 0, 0, 0],\n",
       "         [0, 0, 0, ..., 0, 0, 0],\n",
       "         [0, 0, 0, ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0, ..., 0, 0, 0],\n",
       "         [0, 0, 0, ..., 0, 0, 0],\n",
       "         [0, 0, 0, ..., 0, 0, 0]], dtype=int32),\n",
       "  'lang_id': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        dtype=int32)},\n",
       " (array([-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "         -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "         -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "         -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1], dtype=int32),\n",
       "  array([2, 0, 1, 2, 0, 1, 0, 1, 2, 1, 0, 2, 0, 2, 1, 0, 1, 2, 0, 1, 2, 1,\n",
       "         2, 0, 0, 2, 1, 0, 2, 1, 2, 0, 1, 0, 2, 1, 0, 2, 1, 2, 1, 0, 1, 0,\n",
       "         2, 2, 1, 0, 0, 2, 1, 1, 0, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 2],\n",
       "        dtype=int32)))"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iterat = sw_impulse.as_numpy_iterator()\n",
    "iterat.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strategy = tf.distribute.TPUStrategy(tpu)\n",
    "train_data_size = c\n",
    "steps_per_epoch = int(train_data_size / batch_size)\n",
    "num_train_steps = steps_per_epoch * epochs\n",
    "warmup_steps = int(epochs * train_data_size * 0.1 / batch_size)\n",
    "\n",
    "# creates an optimizer with learning rate schedule\n",
    "optimizer = nlp.optimization.create_optimizer(\n",
    "    2e-5, num_train_steps=num_train_steps, num_warmup_steps=warmup_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with strategy.scope():\n",
    "    model.save('gs://nts2020/XNLI_SW_uniform_10epochs_tf',save_format='tf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with strategy.scope():\n",
    "    loaded_model = tf.keras.models.load_model('gs://nts2020/XNLI_SW_uniform_10epochs_tf', \n",
    "                                             compile=False,custom_objects={\"accuracy_mod\": accuracy_mod})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delta = 0.20\n",
    "\n",
    "for i in range(0, len(train_sampling_factor)):\n",
    "    train_sampling_factor[i] = train_sampling_factor[i] * (1 - delta)\n",
    "train_sampling_factor[1] += delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if sum(train_sampling_factor)!=1:\n",
    "  train_sampling_factor[1]+= 1- sum(train_sampling_factor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sampling_factor = [0.03,\n",
    " 0.08882590708500053,\n",
    " 0.06428571428571428,\n",
    " 0.06428571428571428,\n",
    " 0.06428571428571428,\n",
    " 0.06428571428571428,\n",
    " 0.06428571428571428,\n",
    " 0.06428571428571428,\n",
    " 0.06428571428571428,\n",
    " 0.06428571428571428,\n",
    " 0.06428571428571428,\n",
    " 0.06428571428571428,\n",
    " 0.06428571428571428,\n",
    " 0.06428571428571428,\n",
    " 0.06428571428571428,\n",
    " 0.06428571428571428]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.01,\n",
       " 0.4681950400000001,\n",
       " 0.01420429,\n",
       " 0.01311955,\n",
       " 0.01560811,\n",
       " 0.01659967,\n",
       " 0.01972801,\n",
       " 0.01462891,\n",
       " 0.01473086,\n",
       " 0.01443204,\n",
       " 0.0133442,\n",
       " 0.32546586,\n",
       " 0.01524783,\n",
       " 0.01558882,\n",
       " 0.01494559,\n",
       " 0.01416122]"
      ]
     },
     "execution_count": 1112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sampling_factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[0.10267981 0.0625526  0.06249352 0.06242887 0.06256116 0.06243307\n",
      " 0.06230485 0.08465985 0.06258101 0.06269672 0.06235731 0.06264312\n",
      " 0.06255352 0.06243089 0.06262369], shape=(15,), dtype=float64)\n",
      "tf.Tensor(\n",
      "[0.22551834 0.05092506 0.05088173 0.0508343  0.05093134 0.05083738\n",
      " 0.05074327 0.11284001 0.0509459  0.05103074 0.05078178 0.05099144\n",
      " 0.05092574 0.05083578 0.0509772 ], shape=(15,), dtype=float64)\n",
      "tf.Tensor(\n",
      "[0.40431934 0.03526531 0.03523852 0.0352092  0.03526919 0.03521111\n",
      " 0.03515291 0.13265491 0.03527819 0.0353306  0.03517673 0.03530633\n",
      " 0.03767179 0.03521012 0.03770576], shape=(15,), dtype=float64)\n",
      "tf.Tensor(\n",
      "[0.4238397  0.02803608 0.02801637 0.02799479 0.02803893 0.02990628\n",
      " 0.02795336 0.20248316 0.02804555 0.02808411 0.02797089 0.02998112\n",
      " 0.02979741 0.02799546 0.0318568 ], shape=(15,), dtype=float64)\n",
      "tf.Tensor(\n",
      "[0.37729983 0.03143109 0.03020035 0.03036671 0.03070844 0.03227983\n",
      " 0.0318654  0.20770386 0.03052425 0.03187087 0.03141393 0.03435792\n",
      " 0.03338099 0.03105007 0.03554646], shape=(15,), dtype=float64)\n",
      "tf.Tensor(\n",
      "[0.43275098 0.02525745 0.02431186 0.02443988 0.02470265 0.02590753\n",
      " 0.02559031 0.23403927 0.02456105 0.0255945  0.02524429 0.02749232\n",
      " 0.02674852 0.02496508 0.02839431], shape=(15,), dtype=float64)\n",
      "tf.Tensor(\n",
      "[0.45327163 0.02324469 0.02241918 0.02406836 0.02130704 0.02228951\n",
      " 0.02203135 0.2510111  0.02263706 0.02203476 0.02174934 0.02357394\n",
      " 0.02453953 0.02152147 0.02430104], shape=(15,), dtype=float64)\n",
      "tf.Tensor(\n",
      "[0.33496123 0.02201704 0.01991355 0.02130408 0.01897021 0.02414009\n",
      " 0.0195851  0.39551735 0.02009783 0.01958799 0.02066583 0.02088825\n",
      " 0.02169953 0.01915245 0.02149947], shape=(15,), dtype=float64)\n",
      "tf.Tensor(\n",
      "[0.46770429 0.01814829 0.0164875  0.01758707 0.01573778 0.01980926\n",
      " 0.01733391 0.3061642  0.01776845 0.01622911 0.01824872 0.01725894\n",
      " 0.01789857 0.01588285 0.01774104], shape=(15,), dtype=float64)\n",
      "tf.Tensor(\n",
      "[0.56795537 0.01625248 0.01481705 0.01576856 0.01513219 0.01888391\n",
      " 0.01661081 0.22173895 0.01592509 0.01665197 0.0174537  0.01548509\n",
      " 0.01713147 0.01429191 0.01590144], shape=(15,), dtype=float64)\n",
      "tf.Tensor(\n",
      "[0.49833459 0.01699078 0.01454483 0.01544779 0.0148443  0.01837844\n",
      " 0.01624398 0.29059739 0.01666003 0.01739374 0.0170379  0.01621479\n",
      " 0.01673473 0.01500314 0.01557359], shape=(15,), dtype=float64)\n",
      "tf.Tensor(\n",
      "[0.60472649 0.013439   0.01155275 0.01225096 0.01178455 0.01450201\n",
      " 0.01286479 0.22813107 0.01318488 0.01374822 0.01347518 0.01284232\n",
      " 0.0132423  0.01190741 0.01234806], shape=(15,), dtype=float64)\n",
      "tf.Tensor(\n",
      "[0.51686019 0.0123899  0.01068942 0.01132031 0.01089906 0.01334275\n",
      " 0.01187356 0.32888894 0.01216153 0.01266748 0.0124224  0.01185332\n",
      " 0.01221315 0.0110101  0.01140792], shape=(15,), dtype=float64)\n",
      "tf.Tensor(\n",
      "[0.56375412 0.01119166 0.00969037 0.01024863 0.00987605 0.01202813\n",
      " 0.01073695 0.29458311 0.01099067 0.01143568 0.01122024 0.01145044\n",
      " 0.01178908 0.00997432 0.01103055], shape=(15,), dtype=float64)\n",
      "tf.Tensor(\n",
      "[0.51799401 0.01075667 0.00934331 0.00986994 0.0095186  0.01154022\n",
      " 0.01032955 0.3435717  0.011289   0.01173506 0.01151921 0.01099938\n",
      " 0.01131658 0.00961133 0.01060543], shape=(15,), dtype=float64)\n",
      "tf.Tensor(\n",
      "[0.5303499  0.00980002 0.00853781 0.00962366 0.00869477 0.01121265\n",
      " 0.00941938 0.34218415 0.01027345 0.01066932 0.01047786 0.01001601\n",
      " 0.01029795 0.00877775 0.00966532], shape=(15,), dtype=float64)\n",
      "tf.Tensor(\n",
      "[0.60547161 0.00835476 0.00729569 0.00820708 0.00742766 0.00953414\n",
      " 0.00803591 0.28587368 0.00875071 0.00908126 0.00892145 0.00853549\n",
      " 0.00877118 0.0074974  0.00824197], shape=(15,), dtype=float64)\n",
      "tf.Tensor(\n",
      "[0.54268521 0.00785245 0.00687048 0.00771575 0.00699306 0.00894141\n",
      " 0.00755722 0.35520276 0.00821857 0.00852382 0.00837629 0.00801963\n",
      " 0.00823749 0.00705781 0.00774806], shape=(15,), dtype=float64)\n",
      "tf.Tensor(\n",
      "[0.55777953 0.00813216 0.0066746  0.0074824  0.00725531 0.00864857\n",
      " 0.00783129 0.34175225 0.00796156 0.00825193 0.00811164 0.0077721\n",
      " 0.00797956 0.00685387 0.00751322], shape=(15,), dtype=float64)\n",
      "tf.Tensor(\n",
      "[0.50369395 0.00753516 0.00620129 0.00694143 0.00673359 0.00800602\n",
      " 0.00726041 0.40315034 0.00737941 0.00764445 0.00751643 0.00720633\n",
      " 0.00739585 0.00636573 0.00696962], shape=(15,), dtype=float64)\n",
      "tf.Tensor(\n",
      "[0.4821327  0.00693087 0.00571754 0.00639152 0.00620243 0.00735779\n",
      " 0.00668143 0.43213101 0.00678949 0.00703003 0.00691387 0.0066323\n",
      " 0.00680442 0.00586744 0.00641715], shape=(15,), dtype=float64)\n",
      "tf.Tensor(\n",
      "[0.49490338 0.00636174 0.00525982 0.00587252 0.00570079 0.00674827\n",
      " 0.00613561 0.42635617 0.0062336  0.00645157 0.00634634 0.00609104\n",
      " 0.00624713 0.00539623 0.0058958 ], shape=(15,), dtype=float64)\n",
      "tf.Tensor(\n",
      "[0.49929754 0.00588241 0.00487343 0.00543497 0.0052777  0.00623536\n",
      " 0.00567569 0.4278574  0.00576529 0.00596448 0.00586833 0.00563492\n",
      " 0.00577766 0.00499855 0.00545627], shape=(15,), dtype=float64)\n",
      "tf.Tensor(\n",
      "[0.42911539 0.00649208 0.00575774 0.00600393 0.00583214 0.00687649\n",
      " 0.00626667 0.48794135 0.00636439 0.00658151 0.00739066 0.0062222\n",
      " 0.00681303 0.00590401 0.00643841], shape=(15,), dtype=float64)\n",
      "tf.Tensor(\n",
      "[0.66021668 0.00551045 0.00489331 0.00510039 0.00495591 0.00583289\n",
      " 0.00532118 0.26937166 0.00540326 0.00558551 0.0062635  0.00528382\n",
      " 0.00577969 0.00501636 0.0054654 ], shape=(15,), dtype=float64)\n",
      "tf.Tensor(\n",
      "[0.56201673 0.00694727 0.00617727 0.00643586 0.00625546 0.00687942\n",
      " 0.00628269 0.35231311 0.00637849 0.0065911  0.00788413 0.00623907\n",
      " 0.00681744 0.00633096 0.006451  ], shape=(15,), dtype=float64)\n",
      "tf.Tensor(\n",
      "[0.50636991 0.00703144 0.00626228 0.00652087 0.0072352  0.00696376\n",
      " 0.00636774 0.40551189 0.00646353 0.00713145 0.00796387 0.00632411\n",
      " 0.00690193 0.00641601 0.00653601], shape=(15,), dtype=float64)\n",
      "tf.Tensor(\n",
      "[0.61063615 0.00742037 0.00661941 0.00688898 0.00870906 0.00735\n",
      " 0.00672938 0.29277258 0.00729516 0.00803768 0.00896011 0.00713991\n",
      " 0.00728568 0.0067797  0.00737584], shape=(15,), dtype=float64)\n",
      "tf.Tensor(\n",
      "[0.6220184  0.00675057 0.00603052 0.00627309 0.00790477 0.00668739\n",
      " 0.0061295  0.2901208  0.00663815 0.00730412 0.008129   0.00649868\n",
      " 0.00662964 0.00617478 0.0067106 ], shape=(15,), dtype=float64)\n",
      "tf.Tensor(\n",
      "[0.48721925 0.008783   0.00785809 0.00816999 0.00960438 0.00814617\n",
      " 0.00747538 0.40142058 0.00863878 0.00888585 0.00987214 0.00903694\n",
      " 0.00862785 0.00752988 0.00873172], shape=(15,), dtype=float64)\n",
      "tf.Tensor(\n",
      "[0.61822753 0.00745984 0.00668527 0.00694676 0.00814555 0.0069268\n",
      " 0.00636402 0.28716164 0.00733923 0.00754581 0.00836864 0.00767205\n",
      " 0.00733009 0.0064098  0.00741696], shape=(15,), dtype=float64)\n",
      "tf.Tensor(\n",
      "[0.55874015 0.00706654 0.00634221 0.00658698 0.00770598 0.00656831\n",
      " 0.00604116 0.35161378 0.0069539  0.0071468  0.00791365 0.00726462\n",
      " 0.00694536 0.00608408 0.0070265 ], shape=(15,), dtype=float64)\n",
      "tf.Tensor(\n",
      "[0.542529   0.00657056 0.00590441 0.00612971 0.00715726 0.00611252\n",
      " 0.00562704 0.37409854 0.00646708 0.00664428 0.00734753 0.00675244\n",
      " 0.00645923 0.0056666  0.00653379], shape=(15,), dtype=float64)\n",
      "tf.Tensor(\n",
      "[0.59476586 0.00591196 0.00531912 0.00551978 0.00643287 0.00550449\n",
      " 0.00507185 0.33020213 0.00581996 0.00597747 0.00660156 0.00607357\n",
      " 0.00581298 0.00510713 0.00587927], shape=(15,), dtype=float64)\n",
      "tf.Tensor(\n",
      "[0.5400271  0.00560893 0.0050524  0.00524092 0.00609685 0.00522656\n",
      " 0.00481989 0.38877186 0.00552265 0.00567035 0.00625464 0.00576041\n",
      " 0.0055161  0.00485308 0.00557827], shape=(15,), dtype=float64)\n",
      "tf.Tensor(\n",
      "[0.57123835 0.00559144 0.00504258 0.00522865 0.00648583 0.00557024\n",
      " 0.00481288 0.3566058  0.00550644 0.00565194 0.00622665 0.00613228\n",
      " 0.00549999 0.00484568 0.00556125], shape=(15,), dtype=float64)\n",
      "tf.Tensor(\n",
      "[0.59772056 0.00588967 0.00531769 0.00551175 0.00778107 0.0058676\n",
      " 0.00542438 0.3241647  0.00619697 0.00635877 0.00654996 0.00645198\n",
      " 0.00579446 0.00511218 0.00585824], shape=(15,), dtype=float64)\n",
      "tf.Tensor(\n",
      "[0.50927221 0.00563437 0.00509295 0.00527678 0.00741595 0.0056135\n",
      " 0.00519402 0.41602763 0.00592474 0.00607748 0.00625785 0.00616543\n",
      " 0.00554433 0.00489811 0.00560464], shape=(15,), dtype=float64)\n",
      "tf.Tensor(\n",
      "[0.50283964 0.00524301 0.00474394 0.0049135  0.00687817 0.0052238\n",
      " 0.00483719 0.42767189 0.00551026 0.00565072 0.00581648 0.00573156\n",
      " 0.00516009 0.0045641  0.00521564], shape=(15,), dtype=float64)\n",
      "tf.Tensor(\n",
      "[0.5985363  0.00477458 0.00432351 0.00447684 0.00624744 0.00475722\n",
      " 0.00440784 0.33820014 0.00501582 0.00514253 0.005292   0.00521544\n",
      " 0.00469968 0.0041608  0.00474986], shape=(15,), dtype=float64)\n",
      "tf.Tensor(\n",
      "[0.71027479 0.00464827 0.00421315 0.00436115 0.00606328 0.00463154\n",
      " 0.00429456 0.22647454 0.00488064 0.00570851 0.0054975  0.00541884\n",
      " 0.00457607 0.00433271 0.00462444], shape=(15,), dtype=float64)\n",
      "tf.Tensor(\n",
      "[0.59166948 0.00559613 0.00542332 0.00525365 0.00777444 0.00557619\n",
      " 0.00552717 0.33020542 0.005873   0.00685719 0.00705741 0.00651322\n",
      " 0.00588598 0.00521969 0.00556772], shape=(15,), dtype=float64)\n",
      "tf.Tensor(\n",
      "[0.68973465 0.00484086 0.00469261 0.00454697 0.00670298 0.00482376\n",
      " 0.00478171 0.24273642 0.00507823 0.00592039 0.0060914  0.00562634\n",
      " 0.00508936 0.00451781 0.0048165 ], shape=(15,), dtype=float64)\n",
      "tf.Tensor(\n",
      "[0.58009328 0.00500306 0.00485103 0.0047016  0.00690632 0.00498552\n",
      " 0.00494241 0.35016526 0.00524633 0.00610787 0.00628253 0.00580733\n",
      " 0.00525772 0.00467168 0.00497808], shape=(15,), dtype=float64)\n",
      "tf.Tensor(\n",
      "[0.55719572 0.00527178 0.00511322 0.0052955  0.00724807 0.00561192\n",
      " 0.00520854 0.36787413 0.00590225 0.006859   0.0066021  0.00610886\n",
      " 0.00553714 0.00492604 0.00524573], shape=(15,), dtype=float64)\n",
      "tf.Tensor(\n",
      "[0.6337904  0.00471275 0.00457225 0.00473376 0.00645753 0.00501389\n",
      " 0.00465672 0.29929162 0.00527065 0.00611498 0.00588854 0.00545322\n",
      " 0.00494772 0.00440629 0.00468967], shape=(15,), dtype=float64)\n",
      "tf.Tensor(\n",
      "[0.55858091 0.00462043 0.00448385 0.00464085 0.00631066 0.00491294\n",
      " 0.00456597 0.37587377 0.00516208 0.00597968 0.00576066 0.00533909\n",
      " 0.00484869 0.00432242 0.004598  ], shape=(15,), dtype=float64)\n",
      "tf.Tensor(\n",
      "[0.54951495 0.00436981 0.00424163 0.00438897 0.00595107 0.00464413\n",
      " 0.00431871 0.38854746 0.00487755 0.00564216 0.00543755 0.00504327\n",
      " 0.0045839  0.00409006 0.00434877], shape=(15,), dtype=float64)\n",
      "tf.Tensor(\n",
      "[0.58630421 0.00512879 0.00531944 0.00482207 0.00743636 0.00509966\n",
      " 0.00474559 0.34349806 0.00535334 0.00618254 0.00596091 0.00553329\n",
      " 0.00503416 0.00480328 0.00477831], shape=(15,), dtype=float64)\n",
      "tf.Tensor(\n",
      "[0.66625724 0.00531296 0.00588405 0.00499847 0.00766594 0.00528311\n",
      " 0.00491998 0.25874036 0.00554294 0.00639031 0.00658467 0.00653523\n",
      " 0.00595199 0.00497919 0.00495356], shape=(15,), dtype=float64)\n",
      "tf.Tensor(\n",
      "[0.62959588 0.00618091 0.00780181 0.0058189  0.01012629 0.00656593\n",
      " 0.00572848 0.27975187 0.00644533 0.00741737 0.00763983 0.00758325\n",
      " 0.00738679 0.0057967  0.00616065], shape=(15,), dtype=float64)\n",
      "tf.Tensor(\n",
      "[0.49147496 0.00763277 0.00960148 0.00673193 0.01160908 0.0075842\n",
      " 0.00662858 0.4006573  0.00744679 0.00913578 0.00940536 0.00874049\n",
      " 0.00851758 0.00670655 0.00812715], shape=(15,), dtype=float64)\n",
      "tf.Tensor(\n",
      "[0.5522697  0.00726415 0.00972072 0.00641902 0.01095602 0.00721866\n",
      " 0.00675317 0.34362084 0.00708993 0.00925834 0.00891768 0.00829893\n",
      " 0.00809109 0.00639516 0.00772658], shape=(15,), dtype=float64)\n",
      "tf.Tensor(\n",
      "[0.4656979  0.00681899 0.00908811 0.00603405 0.01022216 0.0067768\n",
      " 0.00634467 0.4367278  0.00665736 0.00866242 0.00834838 0.00777706\n",
      " 0.00758489 0.00601186 0.00724755], shape=(15,), dtype=float64)\n",
      "tf.Tensor(\n",
      "[0.50347832 0.0062055  0.0082371  0.00549887 0.00924626 0.00616757\n",
      " 0.00577874 0.40786586 0.00606017 0.00785723 0.00757662 0.00706531\n",
      " 0.00689309 0.00547887 0.00659048], shape=(15,), dtype=float64)\n",
      "tf.Tensor(\n",
      "[0.6326263  0.00539934 0.00714302 0.00479009 0.0080048  0.00536667\n",
      " 0.00503156 0.29033578 0.00527414 0.00681788 0.00657743 0.00613873\n",
      " 0.0059908  0.00477282 0.00573065], shape=(15,), dtype=float64)\n",
      "tf.Tensor(\n",
      "[0.63224457 0.00542152 0.007146   0.00514454 0.00799358 0.00538909\n",
      " 0.00505615 0.28926757 0.00529721 0.00729109 0.00703752 0.00615433\n",
      " 0.0060079  0.00479877 0.00575017], shape=(15,), dtype=float64)\n",
      "tf.Tensor(\n",
      "[0.63253938 0.00505228 0.00663889 0.00479653 0.00741511 0.00502235\n",
      " 0.00471487 0.29441882 0.00493754 0.00677193 0.00653937 0.00572771\n",
      " 0.00559288 0.00447692 0.00535542], shape=(15,), dtype=float64)\n",
      "tf.Tensor(\n",
      "[0.56677853 0.00493738 0.00647026 0.0046895  0.00721714 0.00490838\n",
      " 0.00461031 0.36192765 0.00482619 0.00659841 0.00637436 0.00559095\n",
      " 0.00546061 0.00437942 0.0052309 ], shape=(15,), dtype=float64)\n",
      "tf.Tensor(\n",
      "[0.55073599 0.00586869 0.007754   0.0056688  0.00868829 0.00584593\n",
      " 0.00557102 0.36365857 0.00588312 0.00818309 0.00745693 0.00660485\n",
      " 0.00663377 0.00529112 0.00615583], shape=(15,), dtype=float64)\n",
      "tf.Tensor(\n",
      "[0.66447894 0.00523904 0.00689457 0.00506274 0.00771008 0.00521897\n",
      " 0.00497644 0.2585452  0.00525177 0.00726951 0.00663459 0.00588706\n",
      " 0.00591248 0.00505186 0.00586675], shape=(15,), dtype=float64)\n",
      "tf.Tensor(\n",
      "[0.49767955 0.00715211 0.00878025 0.0064724  0.01047067 0.00712501\n",
      " 0.00679738 0.39995652 0.00671138 0.00925042 0.00903059 0.00802576\n",
      " 0.00754519 0.00645865 0.00854411], shape=(15,), dtype=float64)\n",
      "tf.Tensor(\n",
      "[0.56724943 0.00636217 0.00778786 0.00576452 0.00925928 0.00633837\n",
      " 0.00605044 0.34183938 0.00597482 0.00819801 0.00800633 0.00712822\n",
      " 0.00670714 0.00575241 0.0075816 ], shape=(15,), dtype=float64)\n",
      "tf.Tensor(\n",
      "[0.61953403 0.00570876 0.00697161 0.00517759 0.00826871 0.00568763\n",
      " 0.00543184 0.29899775 0.00536462 0.00733381 0.0071646  0.00638806\n",
      " 0.00601488 0.00516682 0.00678928], shape=(15,), dtype=float64)\n",
      "tf.Tensor(\n",
      "[0.58083289 0.00727102 0.00776071 0.00578551 0.00980573 0.00678174\n",
      " 0.00606635 0.32535085 0.00599213 0.00815766 0.00851622 0.00711988\n",
      " 0.00670923 0.0057736  0.00807649], shape=(15,), dtype=float64)\n",
      "tf.Tensor(\n",
      "[0.52181046 0.00714277 0.00761594 0.00570132 0.00958134 0.006669\n",
      " 0.00597453 0.38503088 0.00590235 0.00854451 0.00834403 0.00747387\n",
      " 0.00659871 0.00568973 0.00792055], shape=(15,), dtype=float64)\n",
      "tf.Tensor(\n",
      "[0.66175422 0.00604295 0.00643824 0.00483493 0.00807349 0.00564654\n",
      " 0.00506435 0.25945924 0.00500376 0.00721215 0.00704526 0.00631962\n",
      " 0.00558768 0.0048252  0.00669237], shape=(15,), dtype=float64)\n",
      "tf.Tensor(\n",
      "[0.46310389 0.0081728  0.00870014 0.00655572 0.01161406 0.00931662\n",
      " 0.00686346 0.42466637 0.00773924 0.01039389 0.01015695 0.00912478\n",
      " 0.00756433 0.00698904 0.00903871], shape=(15,), dtype=float64)\n",
      "tf.Tensor(\n",
      "[0.44978965 0.00972122 0.01078902 0.00786817 0.01428687 0.0114788\n",
      " 0.00810001 0.41386098 0.00947655 0.01255188 0.01203044 0.01133675\n",
      " 0.00909163 0.0086435  0.01097453], shape=(15,), dtype=float64)\n",
      "tf.Tensor(\n",
      "[0.48799176 0.00865332 0.00958691 0.00702527 0.01262197 0.01018823\n",
      " 0.00722951 0.39085276 0.00843893 0.01112096 0.01066814 0.01006451\n",
      " 0.0081013  0.00770768 0.00974876], shape=(15,), dtype=float64)\n",
      "tf.Tensor(\n",
      "[0.47158962 0.00855158 0.00945553 0.0074419  0.01236946 0.01003586\n",
      " 0.00716603 0.40638425 0.00834349 0.01093308 0.01049794 0.01131589\n",
      " 0.00801539 0.00763229 0.01026767], shape=(15,), dtype=float64)\n",
      "tf.Tensor(\n",
      "[0.59672517 0.00726565 0.00802026 0.00633582 0.01043554 0.00850338\n",
      " 0.00610406 0.29980916 0.00709158 0.00924825 0.00888731 0.0095653\n",
      " 0.00681684 0.00649563 0.00869607], shape=(15,), dtype=float64)\n",
      "tf.Tensor(\n",
      "[0.67642012 0.00621598 0.00685304 0.0054288  0.00888134 0.00726005\n",
      " 0.00523223 0.23519507 0.0060688  0.00788631 0.00758304 0.0081524\n",
      " 0.00583633 0.00556427 0.00742221], shape=(15,), dtype=float64)\n",
      "tf.Tensor(\n",
      "[0.58872743 0.00621258 0.00684152 0.00543345 0.00883442 0.00724259\n",
      " 0.00523854 0.32305651 0.00606707 0.00785857 0.00756045 0.00811988\n",
      " 0.00583709 0.00556769 0.00740222], shape=(15,), dtype=float64)\n",
      "tf.Tensor(\n",
      "[0.44766528 0.00641687 0.00705711 0.00562136 0.00907453 0.0074645\n",
      " 0.00542194 0.45989894 0.0062685  0.00808882 0.00831814 0.00835317\n",
      " 0.00644548 0.00575861 0.00814677], shape=(15,), dtype=float64)\n",
      "tf.Tensor(\n",
      "[0.59893573 0.00622115 0.00683262 0.00545907 0.00874852 0.00722083\n",
      " 0.00562702 0.3099087  0.00649398 0.00781446 0.00803212 0.00861562\n",
      " 0.00624851 0.00597217 0.00786948], shape=(15,), dtype=float64)\n",
      "tf.Tensor(\n",
      "[0.64547383 0.00558255 0.00612531 0.00490461 0.00781911 0.00646936\n",
      " 0.00505416 0.27285129 0.00582485 0.00699463 0.00718697 0.00770195\n",
      " 0.00560686 0.00536124 0.00704327], shape=(15,), dtype=float64)\n",
      "tf.Tensor(\n",
      "[0.52613182 0.00697731 0.0071585  0.0057467  0.00972661 0.00755509\n",
      " 0.00592005 0.37520512 0.00727642 0.00871615 0.00895219 0.00958323\n",
      " 0.00655978 0.00627567 0.00821535], shape=(15,), dtype=float64)\n",
      "tf.Tensor(\n",
      "[0.54490439 0.00633985 0.00650239 0.00523313 0.00879477 0.00685778\n",
      " 0.00538933 0.36557914 0.00660811 0.00789533 0.00810573 0.00866735\n",
      " 0.0059649  0.00570945 0.00744833], shape=(15,), dtype=float64)\n",
      "tf.Tensor(\n",
      "[0.52170486 0.0058889  0.00603813 0.0048705  0.00813354 0.00636411\n",
      " 0.00501447 0.39525537 0.00613515 0.00731344 0.00750552 0.00801752\n",
      " 0.00554432 0.0053093  0.00690488], shape=(15,), dtype=float64)\n",
      "tf.Tensor(\n",
      "[0.50199275 0.00625859 0.00641517 0.00518741 0.00919025 0.00675686\n",
      " 0.0057034  0.40751471 0.00651691 0.00827792 0.00794951 0.00848263\n",
      " 0.00589668 0.00603497 0.00782223], shape=(15,), dtype=float64)\n",
      "tf.Tensor(\n",
      "[0.53117392 0.00614384 0.00629547 0.00510383 0.00957778 0.00662599\n",
      " 0.00560539 0.37846306 0.00639393 0.00809156 0.00777594 0.00885354\n",
      " 0.00579299 0.00592711 0.00817566], shape=(15,), dtype=float64)\n",
      "tf.Tensor(\n",
      "[0.59867553 0.0077986  0.00655353 0.00532643 0.00990151 0.00689279\n",
      " 0.00584368 0.30437913 0.00710867 0.00839135 0.00806941 0.00979222\n",
      " 0.00603685 0.00617488 0.00905543], shape=(15,), dtype=float64)\n",
      "tf.Tensor(\n",
      "[0.59315339 0.00710368 0.00598429 0.00487561 0.00898173 0.00628986\n",
      " 0.0053436  0.31852922 0.00648409 0.00763465 0.00734643 0.00888452\n",
      " 0.00551814 0.00564276 0.00822802], shape=(15,), dtype=float64)\n",
      "tf.Tensor(\n",
      "[0.61931283 0.00729733 0.00658238 0.00503214 0.00981705 0.00647243\n",
      " 0.00550971 0.28740085 0.00666956 0.00893942 0.00754282 0.00909245\n",
      " 0.00568757 0.00621119 0.00843226], shape=(15,), dtype=float64)\n",
      "tf.Tensor(\n",
      "[0.53534184 0.00799151 0.00771201 0.00553727 0.01069387 0.0071005\n",
      " 0.00605667 0.36002412 0.00731372 0.00975592 0.00825607 0.00991972\n",
      " 0.00667624 0.00777971 0.00984083], shape=(15,), dtype=float64)\n",
      "tf.Tensor(\n",
      "[0.61235386 0.0074376  0.00718171 0.00518025 0.01057119 0.0066208\n",
      " 0.00565995 0.28854379 0.00681654 0.00904594 0.00767954 0.00919464\n",
      " 0.00623078 0.00773794 0.00974547], shape=(15,), dtype=float64)\n",
      "tf.Tensor(\n",
      "[0.70000003 0.00617218 0.00596285 0.00431815 0.00871837 0.00550324\n",
      " 0.00471354 0.21784    0.00566375 0.00748302 0.0063699  0.00760379\n",
      " 0.00518306 0.0064176  0.00805053], shape=(15,), dtype=float64)\n",
      "tf.Tensor(\n",
      "[0.5824751  0.00650066 0.00628201 0.00455966 0.00915001 0.0058015\n",
      " 0.00497443 0.3310405  0.00596937 0.00786695 0.00670707 0.00799258\n",
      " 0.00546639 0.00675684 0.00845692], shape=(15,), dtype=float64)\n",
      "tf.Tensor(\n",
      "[0.60757915 0.00745256 0.00631423 0.00459975 0.00976508 0.00583718\n",
      " 0.00501377 0.3035258  0.00600395 0.00841856 0.00673542 0.0080046\n",
      " 0.0055039  0.00678469 0.00846134], shape=(15,), dtype=float64)\n",
      "tf.Tensor(\n",
      "[0.5808946  0.00771458 0.00655196 0.00479024 0.01005916 0.00606305\n",
      " 0.00557278 0.32528905 0.00665942 0.00869678 0.00698279 0.00884104\n",
      " 0.00611122 0.00703314 0.00874019], shape=(15,), dtype=float64)\n",
      "tf.Tensor(\n",
      "[0.53842152 0.00891505 0.00810797 0.00594997 0.01235623 0.00751071\n",
      " 0.00646916 0.34814725 0.00771286 0.01071353 0.00808187 0.01088793\n",
      " 0.00708614 0.00813928 0.01150054], shape=(15,), dtype=float64)\n",
      "tf.Tensor(\n",
      "[0.51935285 0.00915226 0.00890677 0.00614665 0.01259313 0.00773345\n",
      " 0.00667568 0.36083708 0.0084798  0.01170444 0.00831152 0.01189059\n",
      " 0.00730284 0.00836954 0.01254338], shape=(15,), dtype=float64)\n",
      "tf.Tensor(\n",
      "[0.38767956 0.00908968 0.00885046 0.0061435  0.01241647 0.00770362\n",
      " 0.00666481 0.49019707 0.00843379 0.01156193 0.00883355 0.01174119\n",
      " 0.00728129 0.00832607 0.015077  ], shape=(15,), dtype=float64)\n",
      "tf.Tensor(\n",
      "[0.55248657 0.00794882 0.00774305 0.00540175 0.01079151 0.00675401\n",
      " 0.00585447 0.34090543 0.0073842  0.01006469 0.0077285  0.01021735\n",
      " 0.00638872 0.00729134 0.01303959], shape=(15,), dtype=float64)\n",
      "tf.Tensor(\n",
      "[0.55841382 0.0088265  0.00918862 0.00603055 0.01191134 0.00803169\n",
      " 0.00652972 0.31770893 0.00876942 0.01188526 0.00858584 0.01206165\n",
      " 0.00760316 0.00810767 0.01634583], shape=(15,), dtype=float64)\n",
      "tf.Tensor(\n",
      "[0.54614936 0.00897353 0.0099714  0.00658809 0.01203112 0.00873724\n",
      " 0.00667077 0.32547855 0.00952492 0.01200544 0.00932915 0.01217907\n",
      " 0.00774981 0.00825525 0.01635629], shape=(15,), dtype=float64)\n",
      "tf.Tensor(\n",
      "[0.62442754 0.00762528 0.00845817 0.0056221  0.01016792 0.00742762\n",
      " 0.00569182 0.26679344 0.00808588 0.01014668 0.00792245 0.01029025\n",
      " 0.0065998  0.0070239  0.01371715], shape=(15,), dtype=float64)\n",
      "tf.Tensor(\n",
      "[0.57677605 0.00717024 0.00794119 0.0053062  0.00951632 0.00698692\n",
      " 0.00537131 0.32117988 0.00759688 0.00949682 0.00744558 0.00962863\n",
      " 0.00621772 0.00661209 0.01275417], shape=(15,), dtype=float64)\n",
      "tf.Tensor(\n",
      "[0.6434173  0.00867361 0.00897804 0.0056472  0.0107231  0.00791511\n",
      " 0.00571571 0.24399796 0.00804606 0.01001806 0.00788834 0.01015429\n",
      " 0.00705518 0.00749642 0.01427361], shape=(15,), dtype=float64)\n"
     ]
    }
   ],
   "source": [
    "new_weights = get_new_static_weights(0.66, tf.Variable(train_sampling_factor[1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.15863377, 0.05279175, 0.06050166, 0.05709644, 0.05539879,\n",
       "       0.0476955 , 0.05519443, 0.10845616, 0.06775053, 0.05859151,\n",
       "       0.05222267, 0.05620628, 0.0563138 , 0.06068141, 0.05246529],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_weights.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.26438723, 0.0416121 , 0.03809939, 0.04833189, 0.04568637,\n",
       "       0.03971065, 0.04196758, 0.16714864, 0.05173302, 0.04493313,\n",
       "       0.03946248, 0.0451804 , 0.04849535, 0.0472131 , 0.03603868])"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_weights.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.38619243, 0.02560933, 0.02769258, 0.02933084, 0.02620112,\n",
       "       0.02557785, 0.02884727, 0.22202005, 0.0365713 , 0.03394441,\n",
       "       0.02719801, 0.03270463, 0.03757767, 0.0339267 , 0.02660581])"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_weights.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.45440307, 0.01953065, 0.01855943, 0.0195817 , 0.0186784 ,\n",
       "       0.0166871 , 0.01911363, 0.28467079, 0.02146498, 0.02363168,\n",
       "       0.01883129, 0.02539632, 0.02050875, 0.01897668, 0.01996551])"
      ]
     },
     "execution_count": 276,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_weights.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.51895109, 0.01334333, 0.01239848, 0.01136344, 0.01348014,\n",
       "       0.01143068, 0.00937475, 0.31597696, 0.0137975 , 0.0156447 ,\n",
       "       0.0102165 , 0.01478939, 0.01335806, 0.0113958 , 0.01447918])"
      ]
     },
     "execution_count": 299,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_weights.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.54957585, 0.00897124, 0.00887785, 0.00799495, 0.0105019 ,\n",
       "       0.00910511, 0.00809789, 0.33139596, 0.00878845, 0.0100313 ,\n",
       "       0.00971826, 0.00991482, 0.00882816, 0.00819859, 0.00999967])"
      ]
     },
     "execution_count": 315,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_weights.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sampling_factor = 15 * [None]\n",
    "train_sampling_factor[0] = 0.01\n",
    "train_sampling_factor[1:] = new_weights.numpy()\n",
    "# train_sampling_factor[1:] = [0.49758378, 0.01234279, 0.01297586, 0.01206473, 0.0130334 ,\n",
    "#        0.01228484, 0.01269346, 0.0109312 , 0.01205256, 0.01229929,\n",
    "#        0.34325856, 0.01188709, 0.01400499, 0.01077152, 0.01181594]\n",
    "# train_sampling_factor[1] -= 0.04\n",
    "if sum(train_sampling_factor)!=1:\n",
    "  train_sampling_factor[1]+= 1- sum(train_sampling_factor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dataset = tf.data.experimental.sample_from_datasets(\n",
    "    [paws_training_dataset, xnli_training_dataset, xnli_ar_training_dataset, xnli_bg_training_dataset, xnli_de_training_dataset, xnli_el_training_dataset, xnli_es_training_dataset, \n",
    "     xnli_fr_training_dataset, xnli_hi_training_dataset, xnli_ru_training_dataset, xnli_sw_training_dataset, xnli_th_training_dataset,\n",
    "     xnli_tr_training_dataset, xnli_ur_training_dataset, xnli_vi_training_dataset, xnli_zh_training_dataset], weights=tf.constant([train_sampling_factor[0], train_sampling_factor[1],\n",
    "                                                                                                                                   train_sampling_factor[2], train_sampling_factor[3],\n",
    "                                                                                                                                   train_sampling_factor[4],train_sampling_factor[5],\n",
    "                                                                                                                                   train_sampling_factor[6],train_sampling_factor[7],\n",
    "                                                                                                                                   train_sampling_factor[8],train_sampling_factor[9],\n",
    "                                                                                                                                   train_sampling_factor[10],train_sampling_factor[11],\n",
    "                                                                                                                                   train_sampling_factor[12],train_sampling_factor[13],\n",
    "                                                                                                                                   train_sampling_factor[14],train_sampling_factor[15]]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": [
    "untouched_dataset = training_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dataset = training_dataset.batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_dataset = tf.data.experimental.sample_from_datasets(\n",
    "    [paws_eval_dataset,xnli_hi_eval_dataset], weights=tf.constant([evaluation_sampling_factor[0],evaluation_sampling_factor[1]]))                                                                                                                              \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_dataset = evaluation_dataset.batch(batch_size, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "   2/1000 [..............................] - ETA: 10:33 - loss: 0.2239 - output1_loss: 4.9653e-04 - output2_loss: 0.2234 - output1_accuracy_mod: 1.0000 - output2_accuracy_mod: 0.8976WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0101s vs `on_train_batch_end` time: 0.5296s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0101s vs `on_train_batch_end` time: 0.5296s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 114s 114ms/step - loss: 0.1840 - output1_loss: 0.0029 - output2_loss: 0.1810 - output1_accuracy_mod: 0.8677 - output2_accuracy_mod: 0.9341 - val_loss: 0.9286 - val_output1_loss: 0.3064 - val_output2_loss: 0.6222 - val_output1_accuracy_mod: 0.7286 - val_output2_accuracy_mod: 0.6297\n",
      "Epoch 2/15\n",
      "1000/1000 [==============================] - 111s 111ms/step - loss: 0.4181 - output1_loss: 0.0037 - output2_loss: 0.4144 - output1_accuracy_mod: 0.8360 - output2_accuracy_mod: 0.8362 - val_loss: 0.7390 - val_output1_loss: 0.2712 - val_output2_loss: 0.4677 - val_output1_accuracy_mod: 0.7191 - val_output2_accuracy_mod: 0.6398\n",
      "Epoch 3/15\n",
      "1000/1000 [==============================] - 111s 111ms/step - loss: 0.5678 - output1_loss: 0.0055 - output2_loss: 0.5623 - output1_accuracy_mod: 0.7442 - output2_accuracy_mod: 0.7692 - val_loss: 0.6588 - val_output1_loss: 0.2271 - val_output2_loss: 0.4317 - val_output1_accuracy_mod: 0.7437 - val_output2_accuracy_mod: 0.6747\n",
      "Epoch 4/15\n",
      "1000/1000 [==============================] - 109s 109ms/step - loss: 0.5694 - output1_loss: 0.0054 - output2_loss: 0.5640 - output1_accuracy_mod: 0.7318 - output2_accuracy_mod: 0.7651 - val_loss: 0.6799 - val_output1_loss: 0.2136 - val_output2_loss: 0.4663 - val_output1_accuracy_mod: 0.7734 - val_output2_accuracy_mod: 0.6562\n",
      "Epoch 5/15\n",
      "1000/1000 [==============================] - 108s 108ms/step - loss: 0.5673 - output1_loss: 0.0050 - output2_loss: 0.5623 - output1_accuracy_mod: 0.7480 - output2_accuracy_mod: 0.7679 - val_loss: 0.6557 - val_output1_loss: 0.2176 - val_output2_loss: 0.4381 - val_output1_accuracy_mod: 0.7799 - val_output2_accuracy_mod: 0.6703\n",
      "Epoch 6/15\n",
      "1000/1000 [==============================] - 108s 108ms/step - loss: 0.5720 - output1_loss: 0.0053 - output2_loss: 0.5667 - output1_accuracy_mod: 0.7291 - output2_accuracy_mod: 0.7648 - val_loss: 0.6264 - val_output1_loss: 0.1845 - val_output2_loss: 0.4419 - val_output1_accuracy_mod: 0.7990 - val_output2_accuracy_mod: 0.6627\n",
      "Epoch 7/15\n",
      "1000/1000 [==============================] - 109s 109ms/step - loss: 0.5759 - output1_loss: 0.0046 - output2_loss: 0.5714 - output1_accuracy_mod: 0.7707 - output2_accuracy_mod: 0.7623 - val_loss: 0.6074 - val_output1_loss: 0.1798 - val_output2_loss: 0.4276 - val_output1_accuracy_mod: 0.8095 - val_output2_accuracy_mod: 0.6851\n",
      "Epoch 8/15\n",
      "1000/1000 [==============================] - 109s 109ms/step - loss: 0.5610 - output1_loss: 0.0044 - output2_loss: 0.5566 - output1_accuracy_mod: 0.7940 - output2_accuracy_mod: 0.7678 - val_loss: 0.6053 - val_output1_loss: 0.1748 - val_output2_loss: 0.4305 - val_output1_accuracy_mod: 0.8216 - val_output2_accuracy_mod: 0.6823\n",
      "Epoch 9/15\n",
      "1000/1000 [==============================] - 110s 110ms/step - loss: 0.5631 - output1_loss: 0.0045 - output2_loss: 0.5586 - output1_accuracy_mod: 0.7583 - output2_accuracy_mod: 0.7702 - val_loss: 0.5956 - val_output1_loss: 0.1713 - val_output2_loss: 0.4243 - val_output1_accuracy_mod: 0.8322 - val_output2_accuracy_mod: 0.6691\n",
      "Epoch 10/15\n",
      "1000/1000 [==============================] - 110s 110ms/step - loss: 0.5650 - output1_loss: 0.0039 - output2_loss: 0.5612 - output1_accuracy_mod: 0.8251 - output2_accuracy_mod: 0.7669 - val_loss: 0.5792 - val_output1_loss: 0.1594 - val_output2_loss: 0.4198 - val_output1_accuracy_mod: 0.8427 - val_output2_accuracy_mod: 0.6823\n",
      "Epoch 11/15\n",
      "1000/1000 [==============================] - 110s 110ms/step - loss: 0.5627 - output1_loss: 0.0041 - output2_loss: 0.5586 - output1_accuracy_mod: 0.8290 - output2_accuracy_mod: 0.7677 - val_loss: 0.5905 - val_output1_loss: 0.1660 - val_output2_loss: 0.4245 - val_output1_accuracy_mod: 0.8352 - val_output2_accuracy_mod: 0.6799\n",
      "Epoch 12/15\n",
      "1000/1000 [==============================] - 110s 110ms/step - loss: 0.4845 - output1_loss: 0.0043 - output2_loss: 0.4802 - output1_accuracy_mod: 0.8232 - output2_accuracy_mod: 0.8022 - val_loss: 0.5747 - val_output1_loss: 0.1558 - val_output2_loss: 0.4188 - val_output1_accuracy_mod: 0.8437 - val_output2_accuracy_mod: 0.6835\n",
      "Epoch 13/15\n",
      "1000/1000 [==============================] - 109s 109ms/step - loss: 0.4554 - output1_loss: 0.0041 - output2_loss: 0.4513 - output1_accuracy_mod: 0.8094 - output2_accuracy_mod: 0.8154 - val_loss: 0.5809 - val_output1_loss: 0.1534 - val_output2_loss: 0.4275 - val_output1_accuracy_mod: 0.8472 - val_output2_accuracy_mod: 0.6763\n",
      "Epoch 14/15\n",
      "1000/1000 [==============================] - 109s 109ms/step - loss: 0.5217 - output1_loss: 0.0038 - output2_loss: 0.5180 - output1_accuracy_mod: 0.8410 - output2_accuracy_mod: 0.7870 - val_loss: 0.5743 - val_output1_loss: 0.1537 - val_output2_loss: 0.4206 - val_output1_accuracy_mod: 0.8618 - val_output2_accuracy_mod: 0.6819\n",
      "Epoch 15/15\n",
      "1000/1000 [==============================] - 108s 108ms/step - loss: 0.5204 - output1_loss: 0.0035 - output2_loss: 0.5169 - output1_accuracy_mod: 0.8472 - output2_accuracy_mod: 0.7886 - val_loss: 0.5635 - val_output1_loss: 0.1499 - val_output2_loss: 0.4136 - val_output1_accuracy_mod: 0.8593 - val_output2_accuracy_mod: 0.6819\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f0f1b9b0748>"
      ]
     },
     "execution_count": 327,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(training_dataset, batch_size = batch_size, epochs= 15, steps_per_epoch = 1000, validation_data=evaluation_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model.compile(optimizer = optimizer, loss = [_loss_with_filter, _loss_with_filter], metrics = [accuracy_mod])\n",
    "loaded_model.evaluate(evaluation_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model.fit(training_dataset, batch_size = batch_size, epochs= 2, steps_per_epoch = 1000, validation_data=evaluation_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model.fit(training_dataset, batch_size = batch_size, epochs= 8, steps_per_epoch = 1000, validation_data=evaluation_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [],
   "source": [
    "itera = None\n",
    "rl_dataset = untouched_dataset#.batch(batch_size)\n",
    "rl_dataset = rl_dataset.batch(32)\n",
    "itera = rl_dataset.as_numpy_iterator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "def get_batch_lang(iterator):\n",
    "  appearances = defaultdict(int)\n",
    "  for curr in iterator.next()[0]['lang_id']:\n",
    "    appearances[curr] += 1\n",
    "    batch_lang_count = 15 *[None]\n",
    "    for i in range(15):\n",
    "      batch_lang_count[i] = appearances[i]\n",
    "  return batch_lang_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # initializer = tf.keras.initializers.RandomUniform(minval=0., maxval=1.)\n",
    "# # values = initializer(shape=(1, 15))\n",
    "# phi = tf.Variable(train_sampling_factor[1:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initializer = tf.keras.initializers.RandomUniform(minval=0., maxval=1.)\n",
    "# input2 = initializer(shape=(1, 15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.squeeze(tf.nn.softmax(input2, axis = -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input1 = phi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "target_lang = 7\n",
    "cce = tf.keras.losses.CategoricalCrossentropy()\n",
    "d = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train(opt, input1, batch_lang_count, R, loss):\n",
    "#    loss = 0\n",
    "#    with tf.GradientTape() as tape:\n",
    "#         tape.watch(input1)\n",
    "#         for i, val in enumerate(batch_lang_count):\n",
    "#           loss += val * R * cce(tf.one_hot(i, depth =d), tf.squeeze(tf.nn.softmax(input1, axis = -1)))\n",
    "#         gradients = tape.gradient(loss, input1)\n",
    "#    opt.apply_gradients(zip([gradients], [input1]))\n",
    "#    #print(loss)\n",
    "#    print(tf.nn.softmax(input1, axis = -1))\n",
    "#    return input1  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train(opt, input1, batch_lang_count, R, loss, priority, flag):\n",
    "#    loss = 0\n",
    "#    with tf.GradientTape() as tape:\n",
    "#         #tape.watch(input1)\n",
    "#         for i, val in enumerate(batch_lang_count):\n",
    "#             loss += val * (100 - R) * cce(tf.one_hot(i, depth =d), tf.squeeze(tf.nn.softmax(input1, axis = -1))) \n",
    "#    gradients = tape.gradient(loss, input1)\n",
    "#    #print(gradients)\n",
    "#    opt.apply_gradients(zip([gradients], [input1]))\n",
    "#    print(loss)\n",
    "#    print(tf.nn.softmax(input1, axis = -1))\n",
    "#    return input1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = tf.keras.optimizers.SGD(learning_rate = 0.001,clipvalue= 0.001)\n",
    "loss = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_new_static_weights(reward, input1):\n",
    "    epsilon = 0.05\n",
    "    epsilon2 = 0.50\n",
    "    acc = 0\n",
    "#     initializer = tf.keras.initializers.RandomUniform(minval=0., maxval=1.)\n",
    "#     input1 = tf.Variable(initializer(shape=(1, 15))[0])\n",
    "    #input1 = tf.Variable(train_sampling_factor[1:])\n",
    "    flag = 0\n",
    "    batch_data = get_batch_lang(itera)\n",
    "    for i in range(100):\n",
    "        draw = random.uniform(0.0, 1.0)\n",
    "        batch_data = get_batch_lang(itera)\n",
    "        if draw <= epsilon:\n",
    "            #print(\"a\")\n",
    "            for i in range(15):\n",
    "                batch_data[i] = random.random()\n",
    "            \n",
    "        #if draw > epsilon and draw < epsilon2:\n",
    "            #print(\"b\")\n",
    "         #   batch_data = get_batch_lang(itera)\n",
    "\n",
    "           \n",
    "        if draw >= epsilon2:\n",
    "            #print(\"c\")\n",
    "            ans = [i for i in range(0, len(batch_data))]\n",
    "            ind = list(set(ans).difference([0, target_lang]))\n",
    "            for i in ind:\n",
    "                batch_data[i] = 0\n",
    "        input1 = trainstep(opt, input1, batch_data, reward * 100, loss)\n",
    "        acc += tf.nn.softmax(input1, axis = -1)    \n",
    "    return acc/100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initializer = tf.keras.initializers.RandomUniform(minval=0., maxval=1.)\n",
    "# test1 = tf.Variable(initializer(shape=(1, 15))[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "# opt1 = tf.keras.optimizers.SGD(learning_rate = 0.001)\n",
    "\n",
    "# loss = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainstep(opt, input1, batch_lang_count, R, loss):\n",
    "   loss = 0\n",
    "   with tf.GradientTape() as tape:\n",
    "        #tape.watch(input1)\n",
    "        for i, val in enumerate(batch_lang_count):\n",
    "            loss += val * (R) * cce(tf.one_hot(i, depth =d), tf.squeeze(tf.nn.softmax(input1, axis = -1))) \n",
    "        #loss += cce(input1, tf.cast(batch_data, dtype= tf.float64))\n",
    "        print(tf.nn.softmax(input1, axis = -1))\n",
    "\n",
    "   gradients = tape.gradient(loss, input1)\n",
    "   #gradients, _ = tf.clip_by_global_norm(gradients, 5.0)\n",
    "   #print(gradients)\n",
    "   opt.apply_gradients(zip([gradients], [input1]))\n",
    "   return input1\n",
    "   #print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 816,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_sampling_factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(0, 100):\n",
    "#     #if i%10 == 0:\n",
    "#     batch_data = get_batch_lang(itera)\n",
    "#     test1 = trainstep(opt1, test1, batch_data, 0.4 * 100, loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_new_static_weights(reward):\n",
    "#     refresh()\n",
    "#     epsilon = 0.05\n",
    "#     epsilon2 = 0.75\n",
    "#     initializer = tf.keras.initializers.RandomUniform(minval=0., maxval=1.)\n",
    "#     input1 = tf.Variable(initializer(shape=(1, 15))[0])\n",
    "#     flag = 0\n",
    "#     batch_data = get_batch_lang()\n",
    "#     for i in range(100):\n",
    "#         draw = random.uniform(0.0, 1.0)\n",
    "#         #batch_data = get_batch_lang()\n",
    "\n",
    "#         if draw <= epsilon:\n",
    "#             print(\"a\")\n",
    "#             initializer = tf.keras.initializers.RandomUniform(minval=0., maxval=1.)\n",
    "#             input1 = tf.Variable(initializer(shape=(1, 15))[0])\n",
    "#             ans = [i for i in range(0, len(batch_data))]\n",
    "#             rind = list(set(ans).difference(random.sample(range(0, 15), 2)))\n",
    "#             for i in rind:\n",
    "#                 batch_data[i] = 0\n",
    "            \n",
    "#         if draw > epsilon and draw < epsilon2:\n",
    "#             print(\"b\")\n",
    "#             phi = tf.Variable(train_sampling_factor[1:])\n",
    "#             input1 = phi\n",
    "#             ans = [i for i in range(0, len(batch_data))]\n",
    "#             find = list(set(ans).difference(random.sample(range(0, 15), 2)))\n",
    "#             for i in find:\n",
    "#                 batch_data[i] = 0\n",
    "            \n",
    "#         if draw >= epsilon2:\n",
    "#             print(\"c\")\n",
    "#             ans = [i for i in range(0, len(batch_data))]\n",
    "#             ind = list(set(ans).difference([0, target_lang]))\n",
    "#             for i in ind:\n",
    "#                 batch_data[i] = 0\n",
    "#         input1 = trainstep(opt, input1, batch_data, reward * 100, loss)\n",
    "            \n",
    "#     return tf.nn.softmax(input1, axis = -1), max1, max2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paws_batched_eval_data = paws_eval_dataset.batch(batch_size)\n",
    "xnli_batched_eval_data = xnli_eval_dataset.batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(paws_batched_eval_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(xnli_batched_eval_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xnli_ar_eval_dataset = xnli_ar_eval_dataset.batch(batch_size)\n",
    "xnli_bg_eval_dataset = xnli_bg_eval_dataset.batch(batch_size)\n",
    "xnli_de_eval_dataset = xnli_de_eval_dataset.batch(batch_size)\n",
    "xnli_el_eval_dataset = xnli_el_eval_dataset.batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xnli_es_eval_dataset = xnli_es_eval_dataset.batch(batch_size)\n",
    "xnli_fr_eval_dataset = xnli_fr_eval_dataset.batch(batch_size)\n",
    "xnli_hi_eval_dataset = xnli_hi_eval_dataset.batch(batch_size)\n",
    "xnli_ru_eval_dataset = xnli_ru_eval_dataset.batch(batch_size)\n",
    "xnli_sw_eval_dataset = xnli_sw_eval_dataset.batch(batch_size)\n",
    "xnli_th_eval_dataset = xnli_th_eval_dataset.batch(batch_size)\n",
    "xnli_tr_eval_dataset = xnli_tr_eval_dataset.batch(batch_size)\n",
    "xnli_ur_eval_dataset = xnli_ur_eval_dataset.batch(batch_size)\n",
    "xnli_vi_eval_dataset = xnli_vi_eval_dataset.batch(batch_size)\n",
    "xnli_zh_eval_dataset = xnli_zh_eval_dataset.batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(xnli_ar_eval_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(xnli_bg_eval_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(xnli_de_eval_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(xnli_el_eval_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(xnli_es_eval_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(xnli_fr_eval_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(xnli_hi_eval_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(xnli_ru_eval_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(xnli_sw_eval_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(xnli_th_eval_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(xnli_tr_eval_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(xnli_ur_eval_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(xnli_vi_eval_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(xnli_zh_eval_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resolver = tf.distribute.cluster_resolver.TPUClusterResolver.connect(tpu='tpu-quickstart', project = 'moana-intern-fall-2020')\n",
    "# tf.config.experimental_connect_to_cluster(resolver)\n",
    "# tf.tpu.experimental.initialize_tpu_system(resolver)\n",
    "# strategy = tf.distribute.TPUStrategy(resolver)\n",
    "# with strategy.scope():\n",
    "#     max_seq_length = 128\n",
    "#     initializer = tf.keras.initializers.TruncatedNormal(\n",
    "#             stddev=bert_config.initializer_range)\n",
    "#     bert_encoder = bert.bert_models.get_transformer_encoder(\n",
    "#         bert_config, max_seq_length)\n",
    "\n",
    "#     input_word_ids = tf.keras.layers.Input(\n",
    "#       shape=(max_seq_length,), dtype=tf.int32, name='input_word_ids')\n",
    "#     input_mask = tf.keras.layers.Input(\n",
    "#       shape=(max_seq_length,), dtype=tf.int32, name='input_mask')\n",
    "#     input_type_ids = tf.keras.layers.Input(\n",
    "#       shape=(max_seq_length,), dtype=tf.int32, name='input_type_ids')\n",
    "\n",
    "#     bert_model = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_multi_cased_L-12_H-768_A-12/2\",\n",
    "#                                 trainable=True)\n",
    "#     #bert_model = hub.KerasLayer(hub_url_bert, trainable=True)\n",
    "#     pooled_output, seq_output = bert_model([input_word_ids, input_mask, input_type_ids])\n",
    "#     output1 = tf.keras.layers.Dropout(rate=bert_config.hidden_dropout_prob)(\n",
    "#       pooled_output)\n",
    "\n",
    "#     output1 = tf.keras.layers.Dense(\n",
    "#       2, kernel_initializer=initializer, name='output1')(\n",
    "#           output1)\n",
    "\n",
    "#     output2 = tf.keras.layers.Dropout(rate=bert_config.hidden_dropout_prob)(\n",
    "#       pooled_output)\n",
    "\n",
    "#     output2 = tf.keras.layers.Dense(\n",
    "#       3, kernel_initializer=initializer, name='output2')(\n",
    "#           output2)\n",
    "\n",
    "#     model = tf.keras.Model(\n",
    "#           inputs={\n",
    "#               'input_word_ids': input_word_ids,\n",
    "#               'input_mask': input_mask,\n",
    "#               'input_type_ids': input_type_ids\n",
    "#           },\n",
    "#           outputs=[output1, output2])\n",
    "\n",
    "#     # Set up epochs and steps\n",
    "#     epochs = 3\n",
    "#     batch_size = 64\n",
    "#     eval_batch_size = 64\n",
    "\n",
    "#     # get train_data_size from metadata\n",
    "#     train_data_size = c\n",
    "#     steps_per_epoch = int(train_data_size / batch_size)\n",
    "#     num_train_steps = steps_per_epoch * epochs\n",
    "#     warmup_steps = int(epochs * train_data_size * 0.1 / batch_size)\n",
    "\n",
    "#     # creates an optimizer with learning rate schedule\n",
    "#     optimizer = nlp.optimization.create_optimizer(\n",
    "#         2e-5, num_train_steps=num_train_steps, num_warmup_steps=warmup_steps)\n",
    "\n",
    "#         paws_training_dataset = create_classifier_dataset(\n",
    "#     \"gs://nts2020/xtereme/pawsx/train.en.tfrecords\",\n",
    "#     128,\n",
    "#     batch_size,\n",
    "#     task_id = 0,\n",
    "#     is_training=True)\n",
    "\n",
    "    \n",
    "#     xnli_training_dataset = create_classifier_dataset(\n",
    "#     \"gs://nts2020/xtereme/xnli/train.en.tfrecords\",\n",
    "#     128,\n",
    "#     batch_size,\n",
    "#     task_id =1,\n",
    "#     is_training=True)\n",
    "\n",
    "#     paws_eval_dataset = create_classifier_dataset(\n",
    "#     \"gs://nts2020/xtereme/pawsx/eval.en.tfrecords\",\n",
    "#     128,\n",
    "#     batch_size,\n",
    "#     task_id = 0,\n",
    "#     is_training=False)\n",
    "    \n",
    "#     xnli_eval_dataset = create_classifier_dataset(\n",
    "#     \"gs://nts2020/xtereme/xnli/eval.en.tfrecords\",\n",
    "#     128,\n",
    "#     batch_size,\n",
    "#     task_id = 1,\n",
    "#     is_training=False)\n",
    "    \n",
    "#     training_dataset = tf.data.experimental.sample_from_datasets(\n",
    "#     [paws_training_dataset, xnli_training_dataset], weights=tf.constant([sampling_factor[0], sampling_factor[1]]))\n",
    "    \n",
    "#     evaluation_dataset = tf.data.experimental.sample_from_datasets(\n",
    "#     [paws_eval_dataset, xnli_eval_dataset], weights=tf.constant([sampling_factor[0], sampling_factor[1]]))\n",
    "    \n",
    "#     training_dataset = training_dataset.batch(batch_size)\n",
    "#     evaluation_dataset = evaluation_dataset.batch(batch_size)\n",
    "    \n",
    " \n",
    "\n",
    "#     model.compile(optimizer = optimizer, loss = [_loss_with_filter, _loss_with_filter], metrics = [accuracy_mod])\n",
    "#     model.fit(training_dataset, batch_size = batch_size, epochs= 13, steps_per_epoch = 1000, validation_data=evaluation_dataset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 (tensorflow)",
   "language": "python",
   "name": "py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
